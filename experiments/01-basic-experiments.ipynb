{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running an experiment üß™\n",
    "\n",
    "In this notebook, we'll walk through how to run a basic experiment.\n",
    "\n",
    "An experiment consists of three key components: a dataset, a task, and evaluators.\n",
    "- Task: A function that takes an input and generates a response.\n",
    "- Evaluator: A function that compares the model's output against the expected output (along with the input) and returns a value based on specific criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file, overriding existing ones.\n",
    "# Disable override if your environment is defined outside the virtualenv.\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from ddtrace.llmobs import Dataset, Experiment, task, evaluator\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the dataset from Datadog\n",
    "dataset = Dataset.pull(name=\"capitals-of-the-world\")\n",
    "dataset.as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a task function to evaluate the dataset.\n",
    "# The function must accept an `input` parameter, which represents a single dataset row.\n",
    "@task\n",
    "def basic_generate_capital_name(input):\n",
    "    output = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": input}],\n",
    "        temperature=1.4\n",
    "    )\n",
    "    return output.choices[0].message.content\n",
    "\n",
    "# Define the experiment with a descriptive name, dataset, task function, and evaluators.\n",
    "experiment = Experiment(\n",
    "    name=\"gpt-4o-mini\",\n",
    "    dataset=dataset,\n",
    "    task=basic_generate_capital_name,\n",
    "    evaluators=[]  # Empty for now; we‚Äôll add evaluators later\n",
    ")\n",
    "\n",
    "# Run the experiment and retrieve results\n",
    "results = experiment.run()\n",
    "results.as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the results to Datadog\n",
    "results.push()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using custom evaluators üî¨\n",
    "\n",
    "Evaluators are functions that assess the model‚Äôs performance by comparing the expected output with the actual output generated by the model. They receive the following parameters:\n",
    "- input ‚Äì The original input prompt.\n",
    "- output ‚Äì The model's generated response.\n",
    "- expected_output ‚Äì The correct answer from the dataset.\n",
    "\n",
    "The evaluator should return a score or assessment based on a defined criterion.\n",
    "\n",
    "#### Example: Exact Match Evaluator\n",
    "For this experiment, we create a simple equality check to determine whether the model‚Äôs response matches the expected answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom evaluator to check if the model's output matches the expected answer.\n",
    "# Evaluators receive `input`, `output` (model-generated), and `expected_output` (ground truth).\n",
    "# You can modify the logic to support different evaluation methods.\n",
    "@evaluator\n",
    "def exact_match(input, output, expected_output):\n",
    "    return expected_output == output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "    name=\"gpt-4o-mini-with-evals\", \n",
    "    dataset=dataset, \n",
    "    task=basic_generate_capital_name,\n",
    "    evaluators=[exact_match],\n",
    "    config={\"model\": \"gpt-4o-mini\", \"temperature\": 1.4} \n",
    ")\n",
    "\n",
    "results = experiment.run()\n",
    "results.as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Another Evaluator üîç\n",
    "The exact match evaluator may not always work well, especially when the model responds in a conversational format. Even if the answer is correct, slight variations in phrasing can cause mismatches.\n",
    "\n",
    "To address this, we'll add an evaluator that checks whether the expected answer appears within the model's output, allowing for more flexible matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@evaluator\n",
    "def contains_answer(input, output, expected_output):\n",
    "    return expected_output in output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the new evaluator, there's no need to rerun the entire experiment. Instead, we can re-evaluate the existing results using the newly added evaluator.\n",
    "\n",
    "This allows us to efficiently test different evaluation strategies without incurring the computational cost of running the experiment again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = experiment.run_evaluations([exact_match, contains_answer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see now two new columns in the dataframe, `custom_evaluator` and `custom_evaluator_2`.\n",
    "# TODO: Check with jonathan to see if this was fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding another experiment\n",
    "You can create and publish multiple experiments, all of which will be accessible in the Datadog UI. This allows you to compare different models, prompts, or evaluation strategies within the same dataset.\n",
    "\n",
    "In this experiment, we‚Äôll test a smaller model and analyze its performance. We'll be using (OpenRouter's API)[https://openrouter.ai] to generate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a new client for the OpenRouter API.\n",
    "client = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=os.getenv(\"OPENROUTER_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before continuing, let's talk about configurations\n",
    "\n",
    "`config` is a dictionary that can be used to parametrize the experiment. It can be used to change the model, the temperature, the system prompt, and other parameters.\n",
    "\n",
    "**Note:** The `config` parameter is optional. It's a way to record the configuration on each experiment for later analysis. This is useful later on in the UI to compare different configurations.\n",
    "\n",
    "\n",
    "The `config` parameter in the task function will receive the configuration dictionary, which is defined in the `Experiment` object.\n",
    "\n",
    "```python\n",
    "@task\n",
    "def my_function(input, config):\n",
    "    ...\n",
    "\n",
    "Experiment(... task=my_function, config={\"model\": \"gpt-4o-mini\", \"temperature\": 1.4}) # This is how you define the configuration in the experiment\n",
    "```\n",
    "\n",
    "Also, it's a convenient way when you want to do something like a hyperparameter search, which we'll cover in notebook 03.\n",
    "\n",
    "\n",
    "For this case we'll pass the model and the temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a new experiment to get better results, this time we'll add a system prompt to the model and few shot examples.\n",
    "\n",
    "@task\n",
    "def generate_capital_name(input, config):\n",
    "    output = client.chat.completions.create(\n",
    "        model=f\"{config['model_provider']}/{config['model']}\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You will respond only with the name of the capital of the country, nothing else.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Paris\"},\n",
    "            {\"role\": \"user\", \"content\": input}],\n",
    "        temperature=config.get(\"temperature\", 0)\n",
    "    )\n",
    "    return output.choices[0].message.content\n",
    "\n",
    "experiment = Experiment(\n",
    "    name=\"meta-llama-3.2-3b-instruct-0\", \n",
    "    dataset=dataset, \n",
    "    task=generate_capital_name,\n",
    "    evaluators=[exact_match, contains_answer],\n",
    "    config={\"model_provider\": \"meta-llama\", \"model\": \"llama-3.2-3b-instruct\", \"temperature\": 0}\n",
    ")\n",
    "\n",
    "results = experiment.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Awesome! üéâ\n",
    "\n",
    "We've just created three experiments with different models and configurations.\n",
    "\n",
    "This is just the beginning, this task is very simple and it's not the best way to evaluate models, but it's a good starting point to understand how to use the SDK.\n",
    "\n",
    "In the next notebook we'll cover how to use a more complex task to evaluate models with more nuanced evaluators.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Feel free to play around with datasets, tasks, and evaluators to get a better understanding of how to use the SDK."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
