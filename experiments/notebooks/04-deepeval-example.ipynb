{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57bqx9qws05",
   "metadata": {},
   "source": [
    "# Using DeepEval Metrics with Experiments ðŸ”\n",
    "\n",
    "In this notebook, we'll explore how to integrate [DeepEval](https://github.com/confident-ai/deepeval), a popular LLM evaluation framework, with Datadog LLM Observability experiments.\n",
    "\n",
    "DeepEval provides sophisticated evaluation metrics like:\n",
    "- **Answer Relevancy**: Measures if the output is relevant to the input question\n",
    "- **G-Eval**: Customizable LLM-as-a-judge evaluations for any criteria\n",
    "- **Faithfulness**: Ensures outputs stay true to provided context (useful for RAG applications)\n",
    "- And many more!\n",
    "\n",
    "By integrating DeepEval with Datadog experiments, you can combine advanced semantic evaluations with Datadog's experiment tracking and visualization capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2826527d25aba56a",
   "metadata": {},
   "source": [
    "## 0. Set up\n",
    "\n",
    "First, let's import the required libraries and initialize both Datadog LLM Observability and DeepEval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe0584b7547b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file.\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from typing import Dict, Any\n",
    "from ddtrace.llmobs import LLMObs\n",
    "from openai import OpenAI\n",
    "\n",
    "# Import DeepEval components\n",
    "from deepeval.metrics import AnswerRelevancyMetric, GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "\n",
    "\n",
    "LLMObs.enable(\n",
    "    api_key=os.getenv(\"DD_API_KEY\"),\n",
    "    app_key=os.getenv(\"DD_APPLICATION_KEY\"),\n",
    "    project_name=\"Onboarding\",\n",
    "    ml_app=\"Onboarding-ML-App\")\n",
    "\n",
    "oai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "csxihcpz6kh",
   "metadata": {},
   "source": [
    "## 1. Understanding DeepEval Basics\n",
    "\n",
    "Before integrating with Datadog experiments, let's see how DeepEval metrics work.\n",
    "\n",
    "It is possible to use DeepEval metrics in the following way:\n",
    "1. Create a metric instance with a threshold\n",
    "2. Create an `LLMTestCase` with your inputs and outputs\n",
    "3. Call `metric.measure(test_case)` to evaluate\n",
    "4. Access `metric.score` to get the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452e5c2d7f0800f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    # Replace this with the actual output from your LLM application\n",
    "    actual_output=\"We offer a 30-day full refund at no extra costs.\",\n",
    "    retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra costs.\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef9645b81bb845",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_relevancy_metric.measure(test_case)\n",
    "display(answer_relevancy_metric.score)\n",
    "# All metrics also offer an explanation\n",
    "display(answer_relevancy_metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44q8t0keteu",
   "metadata": {},
   "source": [
    "The metric returns a score (typically between 0 and 1) and a `reason` explaining the evaluation. This makes it easy to understand why a particular score was assigned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a32b81ce6e39eee",
   "metadata": {},
   "source": [
    "## 2. Integrating DeepEval with Datadog Experiments\n",
    "\n",
    "Now that we understand how DeepEval works, let's integrate it with Datadog experiments.\n",
    "\n",
    "**The Challenge**: Datadog evaluators receive `input_data`, `output_data`, and `expected_output` as separate parameters, but DeepEval metrics expect a single `LLMTestCase` object.\n",
    "\n",
    "**The Solution**: Create evaluator functions that convert Datadog's format into DeepEval's `LLMTestCase` format, then measure and return the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df081a57f6ed91d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the dataset from Datadog\n",
    "dataset = LLMObs.pull_dataset(dataset_name=\"capitals-of-the-world\")\n",
    "dataset.as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yykdsajjbel",
   "metadata": {},
   "source": [
    "For this example, we'll reuse the `capitals-of-the-world` dataset from the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccfc9b032519a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DeepEval metric instances\n",
    "# These can be reused across multiple evaluations\n",
    "EVAL_MODEL = \"gpt-4o-mini\"\n",
    "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7, model=EVAL_MODEL)\n",
    "\n",
    "correctness_geval_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
    "    model=EVAL_MODEL\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluator functions that bridge Datadog and DeepEval\n",
    "def answer_relevancy_evaluator(input_data, output_data, expected_output):\n",
    "    \"\"\"Evaluates if the output is relevant to the input question.\"\"\"\n",
    "    test_case = LLMTestCase(\n",
    "        input=input_data.get(\"question\", \"\"),\n",
    "        actual_output=output_data\n",
    "    )\n",
    "    answer_relevancy_metric.measure(test_case)\n",
    "    score = answer_relevancy_metric.score\n",
    "    return score\n",
    "\n",
    "\n",
    "def correctness_geval_evaluator(input_data, output_data, expected_output):\n",
    "    \"\"\"Custom G-Eval evaluator for correctness using LLM-as-judge.\"\"\"\n",
    "    test_case = LLMTestCase(\n",
    "        input=input_data.get(\"question\", \"\"),\n",
    "        actual_output=output_data,\n",
    "        expected_output=expected_output\n",
    "    )\n",
    "    correctness_geval_metric.measure(test_case)\n",
    "    score = correctness_geval_metric.score\n",
    "    return score\n",
    "\n",
    "\n",
    "display(\"âœ… DeepEval evaluators created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g4f35m2kunt",
   "metadata": {},
   "source": [
    "## 3. Running an Experiment with DeepEval Metrics\n",
    "\n",
    "Now let's create an experiment that combines traditional evaluators (exact match, substring check) with DeepEval's advanced semantic evaluators.\n",
    "\n",
    "We'll use the same capital cities dataset and task from the previous notebooks, but add DeepEval metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a114720ac4d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task function: Generate capital city names\n",
    "def generate_capital(input_data: Dict[str, Any], config: Dict[str, Any]) -> str:\n",
    "    output = oai_client.chat.completions.create(\n",
    "        model=config[\"model\"],\n",
    "        messages=[{\"role\": \"user\", \"content\": input_data[\"question\"]}],\n",
    "        temperature=config[\"temperature\"]\n",
    "    )\n",
    "    return output.choices[0].message.content\n",
    "\n",
    "\n",
    "# Traditional evaluators for comparison\n",
    "def exact_match(input_data, output_data, expected_output):\n",
    "    return expected_output == output_data\n",
    "\n",
    "\n",
    "def contains_answer(input_data, output_data, expected_output):\n",
    "    return expected_output in output_data\n",
    "\n",
    "\n",
    "# Define the experiment combining traditional and DeepEval evaluators\n",
    "experiment = LLMObs.experiment(\n",
    "    name=\"deepeval-integration-example\",\n",
    "    dataset=dataset,\n",
    "    task=generate_capital,\n",
    "    evaluators=[\n",
    "        exact_match,                    # Traditional: Exact string match\n",
    "        contains_answer,                # Traditional: Substring check\n",
    "        answer_relevancy_evaluator,     # DeepEval: Semantic relevancy\n",
    "        correctness_geval_evaluator     # DeepEval: LLM-as-judge correctness\n",
    "    ],\n",
    "    config={\"model\": \"gpt-4o-mini\", \"temperature\": 0},\n",
    "    description=\"Experiment combining traditional evaluators with DeepEval metrics\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bbe82fb74fc52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = experiment.run(jobs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yrbz0d5333",
   "metadata": {},
   "source": [
    "Let's run the experiment! This will execute the task against each row in the dataset and evaluate using both traditional and DeepEval metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73534d2f81b0d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4exhqugjmo5",
   "metadata": {},
   "source": [
    "In Datadog, you'll see all evaluator results side-by-side:\n",
    "- **Traditional evaluators** (`exact_match`, `contains_answer`) provide binary pass/fail results\n",
    "- **DeepEval evaluators** (`answer_relevancy_evaluator`, `correctness_geval_evaluator`) provide nuanced scores (0-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s61e60kduf",
   "metadata": {},
   "source": [
    "After the experiment completes, you can view it in Datadog (it may take a few seconds to be accessible)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8l7jxx78cj8",
   "metadata": {},
   "source": [
    "## Awesome! ðŸŽ‰\n",
    "\n",
    "You've successfully integrated DeepEval metrics with Datadog LLM Observability experiments! By creating simple bridge functions that convert Datadog's evaluator format to DeepEval's `LLMTestCase` format, you can now use sophisticated LLM evaluation metrics like Answer Relevancy and G-Eval alongside traditional evaluators.\n",
    "\n",
    "You can explore other DeepEval metrics like `FaithfulnessMetric` and `HallucinationMetric` for RAG applications, or create custom G-Eval criteria for domain-specific evaluations.\n",
    "\n",
    "---\n",
    "\n",
    "Feel free to experiment with different DeepEval metrics, adjust thresholds, and combine them with your own custom evaluators to build comprehensive evaluation pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
