{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from ddtrace.llmobs import LLMObs\n",
    "\n",
    "LLMObs.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a weather forecasting agent\n",
    "\n",
    "In the next cells, we build the logic for a simple agent that can answer questions about the weather. The code for the agent is adapted from Peter Roelants's excellent blog post [\"Implement a simple ReAct Agent using OpenAI function calling\"](https://peterroelants.github.io/posts/react-openai-function-calling/).\n",
    "\n",
    "First, we create a system prompt that initiates some basic ReAct agent logic:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant who can answer multistep questions by sequentially calling functions. \n",
    "\n",
    "Follow a pattern of:\n",
    "THOUGHT (reason step-by-step about which function to call next),\n",
    "ACTION (call a function to as a next step towards the final answer), \n",
    "OBSERVATION (output of the function).\n",
    "\n",
    "Reason step by step which actions to take to get to the answer. \n",
    "Only call functions with arguments coming verbatim from the user or the output of other functions.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_initial_messages(question_prompt):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question_prompt,\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some tools that we want the agent to have access to:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "\n",
    "FORECAST_API_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
    "CURRENT_LOCATION_BY_IP_URL = \"http://ip-api.com/json?fields=lat,lon\"\n",
    "\n",
    "\n",
    "def get_current_location():\n",
    "    time.sleep(0.5)  # simulate a longer task\n",
    "    print(requests.get(CURRENT_LOCATION_BY_IP_URL).json())\n",
    "    return json.dumps(requests.get(CURRENT_LOCATION_BY_IP_URL).json())\n",
    "\n",
    "\n",
    "def get_current_weather(latitude, longitude, temperature_unit):\n",
    "    time.sleep(0.3)  # simulate a longer task\n",
    "    resp = requests.get(\n",
    "        FORECAST_API_URL,\n",
    "        params={\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "            \"temperature_unit\": temperature_unit,\n",
    "            \"current_weather\": True,\n",
    "        },\n",
    "    )\n",
    "    return json.dumps(resp.json())\n",
    "\n",
    "\n",
    "def calculate(formula):\n",
    "    return str(eval(formula))\n",
    "\n",
    "\n",
    "class StopException(Exception):\n",
    "    \"\"\"\n",
    "    Signal that the task is finished.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def finish(answer):\n",
    "    raise StopException(answer)\n",
    "\n",
    "\n",
    "available_functions = {\n",
    "    \"get_current_location\": get_current_location,\n",
    "    \"get_current_weather\": get_current_weather,\n",
    "    \"calculate\": calculate,\n",
    "    \"finish\": finish,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a JSON schema in the `function_schema` array to describe each function. We'll pass this to the agent:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_schema = [\n",
    "    {\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_location\",\n",
    "            \"description\": \"Get the current location of the user.\",\n",
    "            \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []},\n",
    "        },\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "    {\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"latitude\": {\"type\": \"number\"},\n",
    "                    \"longitude\": {\"type\": \"number\"},\n",
    "                    \"temperature_unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"latitude\", \"longitude\", \"temperature_unit\"],\n",
    "            },\n",
    "        },\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "    {\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate\",\n",
    "            \"description\": \"Calculate the result of a given formula.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"formula\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Numerical expression to compute the result of, in Python syntax.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"formula\"],\n",
    "            },\n",
    "        },\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "    {\n",
    "        \"function\": {\n",
    "            \"name\": \"finish\",\n",
    "            \"description\": \"Once you have the information required, answer the user's original question, and finish the conversation.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"answer\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Answer to the user's question.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"answer\"],\n",
    "            },\n",
    "        },\n",
    "        \"type\": \"function\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate our OpenAI client:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "oai_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function called `execute_loop_step` that handles the recursive agent logic. In the function, we:\n",
    "\n",
    "1. Call the OpenAI chat completions endpoint with the system and user prompt.\n",
    "2. Execute any tool calls requested by the LLM.\n",
    "3. Add the results of those tool calls to the messages array in the `append_tool_message_and_execute_loop` helper function.\n",
    "4. Call the chat completions endpoint with the new messages array.\n",
    "5. We stop the loop when we either have an answer, or we've reached the `MAX_CALLS` limit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddtrace.llmobs.decorators import *\n",
    "\n",
    "MAX_CALLS = 4\n",
    "MODEL = \"gpt-4\"\n",
    "\n",
    "\n",
    "@workflow()\n",
    "def execute_loop_step(messages, calls_left=MAX_CALLS):\n",
    "\n",
    "    if calls_left < 1:\n",
    "        return messages\n",
    "\n",
    "    # https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages\n",
    "    response = oai_client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        tools=function_schema,\n",
    "    )\n",
    "    response_message = response.choices[0].message\n",
    "    if response_message.content:\n",
    "        print(\"\\n\")\n",
    "        print(response_message.content)\n",
    "    if response_message.tool_calls:\n",
    "        print(\"\\n\")\n",
    "        print(\"CALL TOOL:\", response_message.tool_calls)\n",
    "    messages.append(response_message)\n",
    "    if not response_message.tool_calls:\n",
    "        return execute_loop_step(messages, calls_left - 1)\n",
    "\n",
    "    for tool_call in response_message.tool_calls:\n",
    "        # define a small helper function to reduce repetitive code\n",
    "        def append_tool_message_and_execute_loop(content):\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": content,\n",
    "                }\n",
    "            )\n",
    "            return execute_loop_step(messages, calls_left - 1)\n",
    "\n",
    "        function_name = tool_call.function.name\n",
    "        function_to_call = available_functions[function_name]\n",
    "        if function_to_call is None:\n",
    "            return append_tool_message_and_execute_loop(\n",
    "                f\"Invalid function name: {function_name!r}\"\n",
    "            )\n",
    "        try:\n",
    "            function_args_dict = json.loads(tool_call.function.arguments)\n",
    "        except json.JSONDecodeError as exc:\n",
    "            return append_tool_message_and_execute_loop(\n",
    "                f\"Error decoding function call `{function_name}` arguments {tool_call.function.arguments!r}! Error: {exc!s}\"\n",
    "            )\n",
    "        try:\n",
    "            with LLMObs.tool(function_name):\n",
    "                LLMObs.annotate(input_data=function_args_dict)\n",
    "                try:\n",
    "                    function_response = function_to_call(**function_args_dict)\n",
    "                    LLMObs.annotate(output_data=function_response)\n",
    "                except StopException as answer:\n",
    "                    LLMObs.annotate(output_data=\"StopException\")\n",
    "                    return str(answer)\n",
    "            return append_tool_message_and_execute_loop(function_response)\n",
    "        except Exception as exc:\n",
    "            return append_tool_message_and_execute_loop(\n",
    "                f\"Error calling function `{function_name}`: {type(exc).__name__}: {exc!s}!\"\n",
    "            )\n",
    "    return \"no answer found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the top-level function to take a prompt from a user, call the agent, and return a response:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.datadoghq.com/tracing/llm_observability/sdk/#agent-span\n",
    "@agent()\n",
    "def call_weather_assistant(question_prompt):\n",
    "    LLMObs.annotate(\n",
    "        input_data=question_prompt,\n",
    "    )\n",
    "    messages = get_initial_messages(question_prompt)\n",
    "    answer = execute_loop_step(messages)\n",
    "    LLMObs.annotate(\n",
    "        output_data=answer,\n",
    "    )\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can ask the weather assistant questions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CALL TOOL: [ChatCompletionMessageToolCall(id='call_HfZSK8FEQwmVVYRNqLyh9FfH', function=Function(arguments='{}', name='get_current_location'), type='function')]\n",
      "{'lat': 41.7599, 'lon': -72.7574}\n",
      "\n",
      "\n",
      "THOUGHT\n",
      "Now that I have obtained the user's current location coordinates, I will use these coordinates to get the weather data at that location. The user wants the temperature in Fahrenheit, so I will specify that in the function.\n",
      "\n",
      "ACTION\n",
      "Call the function to get the current weather of the user's location.\n",
      "\n",
      "\n",
      "CALL TOOL: [ChatCompletionMessageToolCall(id='call_md31coobmrNryqsSj0AqgO7s', function=Function(arguments='{\\n\"latitude\": 41.7599,\\n\"longitude\": -72.7574,\\n\"temperature_unit\": \"fahrenheit\"\\n}', name='get_current_weather'), type='function')]\n",
      "\n",
      "\n",
      "THOUGHT\n",
      "Now that I have the weather data, I will respond to the user with the temperature and their location coordinates. \n",
      "\n",
      "ACTION\n",
      "Finish the request and provide the requested data to the user.\n",
      "\n",
      "\n",
      "CALL TOOL: [ChatCompletionMessageToolCall(id='call_NqjgoEklHjHeaLSN3sEcaRnj', function=Function(arguments='{\\n\"answer\": \"The current temperature in your location (41.7599, -72.7574) is 95.6°F.\"\\n}', name='finish'), type='function')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current temperature in your location (41.7599, -72.7574) is 95.6°F.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_weather_assistant(\n",
    "    \"What is the weather in my current location? Please give me the temperature in farenheit. Also tell me my current location coordinates.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the trace in Datadog\n",
    "\n",
    "Now, try checking out the [LLM Observability interface](https://app.datadoghq.com/llm) in Datadog. You should see a trace that describes the agent we just ran.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
