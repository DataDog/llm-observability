{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ragas Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datadog's LLM Observability integration with the [RAGAS](https://docs.ragas.io/en/stable/) (rag assessment) evaluation framework enables developer's to continuously evaluate their RAG applications in production.\n",
    "\n",
    "In the following example, we'll instrument a RAG-powered application and enable the `ragas_faithfulness` evaluator to monitor for hallucinations.\n",
    "\n",
    "#### Learning Goals\n",
    "\n",
    "- Understand how to instrument a RAG application to enable the Ragas integration\n",
    "- Understand how to use Datadog & Ragas to continuously monitor your LLM app in production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why use RAGAS for production monitoring?\n",
    "\n",
    "Ragas scores for RAG performance are powered by LLM-as-a-judge methods. By running Ragas evaluations on your LLM Observability traces, you can \n",
    "1. Filter for traces that have high likelihood of hallucinations and review these traces to improve your augmented generation LLM steps.\n",
    "2. Track scores over time and after critical changes to your RAG application (e.g. prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you've followed the instructions in the `README` file to set up your environment to enable LLM Observability.\n",
    "\n",
    "We'll also need to install some dependencies for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index==\"0.10.42\" ragas --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enabling the Ragas Evaluations\n",
    "\n",
    "Next, enable the Ragas faithfulness evaluations through the `DD_LLMOBS_EVALUATORS` environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"_DD_LLMOBS_EVALUATORS\"] = \"ragas_faithfulness\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable LLM Observability tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddtrace.llmobs import LLMObs\n",
    "\n",
    "LLMObs.enable(ml_app=\"support-ml-obs\", agentless_enabled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your RAG Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be creating a Q&A agent over LLM Observabilites' public documentation.\n",
    "\n",
    "First, fetch LLM Obs documentation [documentation](https://github.com/DataDog/documentation/tree/master) from Datadog's public docs repository. This will act as the knowledge base our RAG app uses to answer questions.\n",
    "\n",
    "Make sure the following source URL and document names are up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_names = [\n",
    "    \"_index\",\n",
    "    \"api\",\n",
    "    \"auto_instrumentation\",\n",
    "    \"core_concepts\",\n",
    "    \"quickstart\",\n",
    "    \"sdk\",\n",
    "    \"span_kinds\",\n",
    "    \"submit_evaluations\",\n",
    "    \"trace_an_llm_application\",\n",
    "]\n",
    "raw_doc_source_url = \"https://raw.githubusercontent.com/DataDog/documentation/master/content/en/llm_observability\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "raw_doc_texts = []\n",
    "for doc_name in doc_names:\n",
    "    doc = requests.get(f\"{raw_doc_source_url}/{doc_name}.md\")\n",
    "    raw_doc_texts.append(Document(text=doc.text))\n",
    "parser = MarkdownNodeParser()\n",
    "base_nodes = parser.get_nodes_from_documents(raw_doc_texts)\n",
    "\n",
    "TOP_K = 2\n",
    "\n",
    "base_index = VectorStoreIndex(base_nodes)\n",
    "base_retriever = base_index.as_retriever(similarity_top_k=TOP_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Instrumentation\n",
    "\n",
    "We'll need to first instrument the data from our LLM calls will be evaluated for hallucations. \n",
    "\n",
    "The following code block contains an LLM call that uses a retrieved ground truths to answer a question.\n",
    "\n",
    "This LLM call will be automatically traced by Datadog's OpenAI integration as an [LLM Span](https://docs.datadoghq.com/llm_observability/terms/#llm-span). This span contains some core information such as the operation's duration, input messages, outputs, invocation parameters, etc. \n",
    "\n",
    "We'll also need to enrich this LLM span with prompt template information that captures the context used for the LLM call. This can be accomplished through the `annotation_context` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddtrace.llmobs import LLMObs\n",
    "from ddtrace.llmobs.decorators import workflow\n",
    "from ddtrace.llmobs.utils import Prompt\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are an engineer meant to answer support questions about a software product.\n",
    "The product is LLM Observability by Datadog, a monitoring solution for LLM applications.\n",
    "\n",
    "You have access to the following reference information: \"{context}\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def augmented_generation(question, context):\n",
    "    with LLMObs.annotation_context(\n",
    "        prompt=Prompt(variables={\"context\": context}),\n",
    "        name=\"augmented_generation\",\n",
    "    ):\n",
    "        answer = (\n",
    "            oai_client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": prompt_template.format(context=context),\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": question,\n",
    "                    },\n",
    "                ],\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "        return answer\n",
    "\n",
    "\n",
    "@workflow\n",
    "def ask_docs(question):\n",
    "    nodes = base_retriever.retrieve(question)\n",
    "    context = \" \".join([node.text for node in nodes])\n",
    "    answer = augmented_generation(question, context)\n",
    "    LLMObs.annotate(input_data=question, output_data=answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** Any span to be evaluated by Ragas Faithfulness Evaluator must be instrumented with a `context` prompt template variable. This is because Ragas faithfulness scores are calculated from a question, answer, and retrieved context. \n",
    "\n",
    "You can also optionally annotate the `question` template variable—if present, we'll use it as the `question` for the faithfulness evaluation. If not present we'll assume the question is the latest message in the input messages array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your RAG app\n",
    "\n",
    "Let's ask our RAG workflow a simple question on how to get started with LLM Observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: To get started with LLM Observability by Datadog, you can follow these steps:\n",
      "\n",
      "1. **Setup Documentation:** Refer to the [Setup documentation][5] for detailed instructions on instrumenting your LLM application with Datadog.\n",
      "\n",
      "2. **Trace an LLM Application:** Follow the [Trace an LLM Application guide][6] to generate a trace using the [LLM Observability SDK for Python][3].\n",
      "\n",
      "3. **Reach Out for Assistance:** If you encounter any issues or have specific questions, feel free to reach out to your account representative for further assistance.\n",
      "\n",
      "Remember that by using LLM Observability, you acknowledge that Datadog shares your company's data with OpenAI LLC for the purpose of providing and improving the product. If you have concerns or wish to opt out of features related to OpenAI, do communicate this to your account representative.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.01.0\n",
      "submitting evaluation\n",
      "\n",
      "submitting evaluation\n",
      "1.0\n",
      "submitting evaluation\n"
     ]
    }
   ],
   "source": [
    "STARTER_QUESTION = \"How do I get started?\"\n",
    "\n",
    "answer = ask_docs(STARTER_QUESTION)\n",
    "\n",
    "print(\"Answer: {}\".format(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at RAGAS evaluation results in Datadog\n",
    "\n",
    "Navigate to the Datadog UI and click on the `ask_docs` trace. You should see the `ragas_faithfulness` custom evaluation joined to the `augmented_generation` LLM span in the trace.\n",
    "\n",
    "Check out the `ragas-support-ml-obs` ml application. This ml application should contain traces of the ragas evaluations themselves—e.g. how exactly the score you saw abovw was generated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
