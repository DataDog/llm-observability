{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling traces for hallucination detection with Ragas \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building off part 1, we'll now add sampling to the Ragas faithfulness evaluator. Sampling allows you to configure how often evaluators run on particular spans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you've followed the instructions in the `README` file to set up your environment to enable LLM Observability.\n",
    "\n",
    "We'll also need to install some dependencies for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index==\"0.10.42\" ragas==\"0.1.21\" --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enabling sampling for Ragas\n",
    "\n",
    "In addition to enabling the `ragas_faithfulness` evaluator, we'll also specify two sampling rules.\n",
    "\n",
    "1. Rule 1 - the `ragas_faithfulness` evaluation should be run 50% percent of the time on the LLM span named `augmented_generation`.\n",
    "\n",
    "`{'sample_rate': 0.5, 'evaluator_label': 'ragas_faithfulness', 'span_name': 'augmented_generation'}`\n",
    "\n",
    "\n",
    "2. Rule 2 - don't run any evaluations on any other LLM spans.\n",
    "\n",
    "`{'sample_rate': 0}`\n",
    "\n",
    "We'll set these rules via the `DD_LLMOBS_EVALUATOR_SAMPLING_RULES` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DD_LLMOBS_EVALUATORS\"] = \"ragas_faithfulness\"\n",
    "os.environ[\"DD_LLMOBS_EVALUATOR_SAMPLING_RULES\"] = (\n",
    "    '[{\"sample_rate\": 0.5, \"evaluator_label\": \"ragas_faithfulness\", \"span_name\": \"augmented_generation\"}, {\"sample_rate\": 0}]'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enabling tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from ddtrace.llmobs import LLMObs\n",
    "\n",
    "LLMObs.enable(ml_app=\"support-bot\", agentless_enabled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create & instrument your RAG Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create & instrument the RAG App just like we did in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_names = [\n",
    "    \"_index\",\n",
    "    \"api\",\n",
    "    \"auto_instrumentation\",\n",
    "    \"core_concepts\",\n",
    "    \"quickstart\",\n",
    "    \"sdk\",\n",
    "    \"span_kinds\",\n",
    "    \"submit_evaluations\",\n",
    "    \"trace_an_llm_application\",\n",
    "]\n",
    "raw_doc_source_url = \"https://raw.githubusercontent.com/DataDog/documentation/master/content/en/llm_observability\"\n",
    "\n",
    "import requests\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "raw_doc_texts = []\n",
    "for doc_name in doc_names:\n",
    "    doc = requests.get(f\"{raw_doc_source_url}/{doc_name}.md\")\n",
    "    raw_doc_texts.append(Document(text=doc.text))\n",
    "parser = MarkdownNodeParser()\n",
    "base_nodes = parser.get_nodes_from_documents(raw_doc_texts)\n",
    "\n",
    "TOP_K = 2\n",
    "\n",
    "base_index = VectorStoreIndex(base_nodes)\n",
    "base_retriever = base_index.as_retriever(similarity_top_k=TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddtrace.llmobs import LLMObs\n",
    "from ddtrace.llmobs.decorators import workflow\n",
    "from ddtrace.llmobs.utils import Prompt\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are an engineer meant to answer support questions about a software product.\n",
    "The product is LLM Observability by Datadog, a monitoring solution for LLM applications.\n",
    "\n",
    "You have access to the following reference information: \"{context}\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def augmented_generation(question, context):\n",
    "    with LLMObs.annotation_context(\n",
    "        prompt=Prompt(variables={\"context\": context}),\n",
    "        name=\"augmented_generation\",\n",
    "    ):\n",
    "        answer = (\n",
    "            oai_client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": prompt_template.format(context=context),\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": question,\n",
    "                    },\n",
    "                ],\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "        return answer\n",
    "\n",
    "\n",
    "@workflow\n",
    "def ask_docs(question):\n",
    "    nodes = base_retriever.retrieve(question)\n",
    "    context = \" \".join([node.text for node in nodes])\n",
    "    answer = augmented_generation(question, context)\n",
    "    LLMObs.annotate(input_data=question, output_data=answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the RAG App\n",
    "\n",
    "Let's use an another LLM to generate a bunch of questions that will be passed into our RAG workflow.\n",
    "\n",
    "This question-generation LLM call will also be auto-instrumented, though there won't be any Ragas faithfulness evaluations tied to the call. For the `augmented_generation` LLM call, only ~50% of them have a Ragas faithfulness score joined to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question():\n",
    "    answer = (\n",
    "        oai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"generate a question about how to setup & best use a SaaS tool to observe LLM-powered applications\",\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "    return answer\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    question = generate_question()\n",
    "    print(f\"Question {i+1}: {question}\")\n",
    "    answer = ask_docs(question)\n",
    "    print(f\"Answer {i+1}: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
