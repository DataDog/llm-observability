{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling traces for hallucination detection with RAGAS \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building off part 1, we'll now add sampling to the ragas faithfulness evaluator. Sampling allows you configure how often evaluators run on particular spans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you've followed the instructions in the `README` file to set up your environment to enable LLM Observability.\n",
    "\n",
    "We'll also need to install some dependencies for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index==\"0.10.42\" ragas --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enabling sampling for Ragas\n",
    "\n",
    "In addition to enabling the `ragas_faithfulness` evaluator, we'll also specify two sampling rules.\n",
    "\n",
    "1. Rule 1 - the `ragas_faithfulness` evaluation should be run 50% percent of the time on the LLM span named `augmented_generation`.\n",
    "\n",
    "`{'sample_rate': 0.5, 'evaluator_label': 'ragas_faithfulness', 'span_name': 'augmented_generation'}`\n",
    "\n",
    "\n",
    "2. Rule 2 - don't run any evaluations on any other llm spans.\n",
    "\n",
    "`{'sample_rate': 0}`\n",
    "\n",
    "We'll set these rules via the `_DD_LLMOBS_EVALUATOR_SAMPLING_RULES` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"_DD_LLMOBS_EVALUATORS\"] = \"ragas_faithfulness\"\n",
    "os.environ[\"_DD_LLMOBS_EVALUATOR_SAMPLING_RULES\"] = (\n",
    "    '[{\"sample_rate\": 0.5, \"evaluator_label\": \"ragas_faithfulness\", \"span_name\": \"augmented_generation\"}, {\"sample_rate\": 0}]'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enabling tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EvaluatorRunnerSamplingRule(sample_rate=0.5, evaluator_label=ragas_faithfulness, span_name=augmented_generation), EvaluatorRunnerSamplingRule(sample_rate=0.0, evaluator_label=<object object at 0x10695be60>, span_name=<object object at 0x10695be60>)]\n",
      "[EvaluatorRunnerSamplingRule(sample_rate=0.5, evaluator_label=ragas_faithfulness, span_name=augmented_generation), EvaluatorRunnerSamplingRule(sample_rate=0.0, evaluator_label=<object object at 0x10695be60>, span_name=<object object at 0x10695be60>)]\n"
     ]
    }
   ],
   "source": [
    "from ddtrace.llmobs import LLMObs\n",
    "\n",
    "LLMObs.enable(ml_app=\"support-bot\", agentless_enabled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create & instrument your RAG Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create & instrument the RAG App just like we did in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_names = [\n",
    "    \"_index\",\n",
    "    \"api\",\n",
    "    \"auto_instrumentation\",\n",
    "    \"core_concepts\",\n",
    "    \"quickstart\",\n",
    "    \"sdk\",\n",
    "    \"span_kinds\",\n",
    "    \"submit_evaluations\",\n",
    "    \"trace_an_llm_application\",\n",
    "]\n",
    "raw_doc_source_url = \"https://raw.githubusercontent.com/DataDog/documentation/master/content/en/llm_observability\"\n",
    "\n",
    "import requests\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "raw_doc_texts = []\n",
    "for doc_name in doc_names:\n",
    "    doc = requests.get(f\"{raw_doc_source_url}/{doc_name}.md\")\n",
    "    raw_doc_texts.append(Document(text=doc.text))\n",
    "parser = MarkdownNodeParser()\n",
    "base_nodes = parser.get_nodes_from_documents(raw_doc_texts)\n",
    "\n",
    "TOP_K = 2\n",
    "\n",
    "base_index = VectorStoreIndex(base_nodes)\n",
    "base_retriever = base_index.as_retriever(similarity_top_k=TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddtrace.llmobs import LLMObs\n",
    "from ddtrace.llmobs.decorators import workflow\n",
    "from ddtrace.llmobs.utils import Prompt\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "oai_client = OpenAI()\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are an engineer meant to answer support questions about a software product.\n",
    "The product is LLM Observability by Datadog, a monitoring solution for LLM applications.\n",
    "\n",
    "You have access to the following reference information: \"{context}\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def augmented_generation(question, context):\n",
    "    with LLMObs.annotation_context(\n",
    "        prompt=Prompt(variables={\"context\": context}),\n",
    "        name=\"augmented_generation\",\n",
    "    ):\n",
    "        answer = (\n",
    "            oai_client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": prompt_template.format(context=context),\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": question,\n",
    "                    },\n",
    "                ],\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "        return answer\n",
    "\n",
    "\n",
    "@workflow\n",
    "def ask_docs(question):\n",
    "    nodes = base_retriever.retrieve(question)\n",
    "    context = \" \".join([node.text for node in nodes])\n",
    "    answer = augmented_generation(question, context)\n",
    "    LLMObs.annotate(input_data=question, output_data=answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the RAG App\n",
    "\n",
    "Let's use an another LLM to generate a bunch of questions that will be passed into our rag workflow.\n",
    "\n",
    "This question-generation LLM call will also be auto-instrumented, though there won't be any Ragas faithfulness evaluations tied to the call. For the `augmented_generation` LLM call, only ~50% of them have a ragas faithfulness score joined to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: How can one effectively set up and utilize a SaaS tool to observe and monitor applications powered by machine learning models (LLM) for optimal performance and efficiency?\n",
      "Answer 1: To effectively set up and utilize a SaaS tool like LLM Observability by Datadog to monitor applications powered by machine learning models (LLM) for optimal performance and efficiency, you can follow these steps:\n",
      "\n",
      "1. **Integration**: Begin by integrating your LLM applications with the monitoring tool. This may involve installing the necessary agents or SDKs provided by the monitoring tool within your application environment.\n",
      "\n",
      "2. **Instrumentation**: Ensure that your machine learning models and LLM applications are properly instrumented to collect relevant metrics and logs. This could include metrics related to cost, latency, performance, usage trends, and other key indicators of application health.\n",
      "\n",
      "3. **Dashboard Configuration**: Leverage the out-of-the-box dashboards provided by the monitoring tool to visualize and track important operational metrics. Customize the dashboards to suit your specific monitoring needs and to gain insights into the performance of your LLM applications.\n",
      "\n",
      "4. **Alerting**: Set up alerting rules based on thresholds defined for critical metrics. This will allow you to proactively address any issues before they impact the performance of your applications.\n",
      "\n",
      "5. **Anomaly Detection**: Utilize features like anomaly detection to automatically identify unusual patterns or behaviors in your LLM applications. This can help you detect issues that may not be immediately apparent through manual monitoring.\n",
      "\n",
      "6. **Cluster Analysis**: Utilize tools within the monitoring platform to identify problematic clusters or areas within your LLM applications. Monitor the quality of responses over time by utilizing features like sentiment analysis and failure detection.\n",
      "\n",
      "7. **Optimization**: Use the insights and data gathered from the monitoring tool to optimize the performance and efficiency of your LLM applications. This may involve tweaking parameters, adjusting resource allocation, or even retraining models based on the feedback received from the monitoring tool.\n",
      "\n",
      "By following these steps and utilizing the features provided by the monitoring tool, you can effectively observe and monitor your applications powered by machine learning models for optimal performance and efficiency.\n",
      "Question 2: What are some key steps to efficiently set up and maximize the usage of a SaaS tool for observing LLM-powered applications?\n",
      "Answer 2: Setting up and maximizing the usage of a SaaS tool like LLM Observability by Datadog for monitoring LLM-powered applications involves several key steps. Here are some best practices to efficiently set up and get the most out of the tool:\n",
      "\n",
      "1. **Define Monitoring Goals**: Clearly outline what you want to achieve by monitoring your LLM-powered applications. Define key metrics, such as cost, latency, performance, and usage trends, that are critical for your business.\n",
      "\n",
      "2. **Install and Configure the Tool**: Install the LLM Observability tool in your environment and configure it to start collecting data from your LLM applications. Ensure that you set up monitoring for all relevant components of your applications.\n",
      "\n",
      "3. **Create Dashboards**: Leverage the out-of-the-box dashboards provided by LLM Observability to monitor operational metrics effectively. Customize these dashboards to suit your specific needs and monitor the performance of your applications in real-time.\n",
      "\n",
      "4. **Set Alerts**: Define alerting rules based on thresholds for key metrics like response time, error rates, or cost overruns. This will help you proactively identify and address issues before they impact your applications.\n",
      "\n",
      "5. **Utilize Clustering and Checks**: Take advantage of features like topical clustering and sentiment analysis to identify problematic clusters within your LLM-powered applications. Monitor the quality of responses over time to ensure a high level of user satisfaction.\n",
      "\n",
      "6. **Regularly Review and Optimize**: Regularly review the monitoring data and dashboards to identify areas for optimization. Use the insights provided by the tool to make informed decisions about resource allocation, performance improvements, and cost optimization.\n",
      "\n",
      "7. **Collaborate and Share Insights**: Share monitoring data and insights with relevant teams within your organization to facilitate collaboration and decision-making. Encourage cross-functional teams to work together to address issues and optimize the performance of your LLM applications.\n",
      "\n",
      "By following these steps and leveraging the capabilities of LLM Observability by Datadog, you can efficiently set up and maximize the usage of the tool for observing your LLM-powered applications.\n",
      "Question 3: How can I effectively set up and utilize a SaaS tool to monitor and analyze LLM-powered applications for optimal performance and efficiency?\n",
      "Answer 3: To effectively set up and utilize the LLM Observability by Datadog SaaS tool to monitor and analyze LLM-powered applications for optimal performance and efficiency, you can follow these steps:\n",
      "\n",
      "1. **Dashboard Setup**:\n",
      "   - Use the out-of-the-box dashboards provided by LLM Observability to monitor cost, latency, performance, and usage trends for all your LLM applications.\n",
      "   - Customize the dashboards to include specific metrics that are relevant to your applications and KPIs.\n",
      "\n",
      "2. **Cluster Monitoring**:\n",
      "   - Utilize the clustering feature to identify problematic clusters within your LLM applications.\n",
      "   - Monitor the quality of responses over time by analyzing metrics like sentiment, failure to answer, and other relevant checks.\n",
      "\n",
      "3. **Alerting and Notifications**:\n",
      "   - Set up alerts based on predefined thresholds for key metrics to proactively identify issues before they impact performance.\n",
      "   - Configure notifications to be sent to relevant team members via email, SMS, or other channels.\n",
      "\n",
      "4. **Anomaly Detection**:\n",
      "   - Leverage anomaly detection capabilities to automatically detect unusual behavior or deviations from normal patterns in your LLM applications.\n",
      "   - Investigate and address anomalies promptly to ensure optimal performance and efficiency.\n",
      "\n",
      "5. **Historical Analysis**:\n",
      "   - Use historical data to analyze trends and patterns in the performance of your LLM applications.\n",
      "   - Identify areas for improvement and optimization based on past performance data.\n",
      "\n",
      "6. **Integration with Other Tools**:\n",
      "   - Integrate LLM Observability with your existing tools and systems to streamline workflows and enhance visibility across your infrastructure.\n",
      "   - Ensure seamless data flow between different monitoring and analytics tools for a comprehensive view of your LLM applications.\n",
      "\n",
      "7. **Regular Review and Optimization**:\n",
      "   - Regularly review the performance metrics and insights provided by LLM Observability to identify opportunities for optimization.\n",
      "   - Actively work on addressing bottlenecks, improving efficiency, and enhancing the overall performance of your LLM-powered applications.\n",
      "\n",
      "By effectively setting up and utilizing the LLM Observability SaaS tool, you can monitor, analyze, and optimize your LLM-powered applications to ensure optimal performance and efficiency.\n",
      "Question 4: What are the best practices for setting up and effectively utilizing a SaaS tool to monitor and analyze LLM-powered applications?\n",
      "Answer 4: Setting up and effectively utilizing a SaaS tool like LLM Observability by Datadog to monitor and analyze LLM-powered applications involves following a few best practices:\n",
      "\n",
      "1. **Define Your Monitoring Objectives**: Clearly define the key metrics and goals you want to monitor. This could include cost optimization, latency reduction, performance improvements, and usage trends.\n",
      "\n",
      "2. **Instrument Your Applications**: Integrate the SaaS monitoring tool with your LLM-powered applications to start collecting data. Ensure that you track relevant data points such as response times, error rates, resource consumption, and user interactions.\n",
      "\n",
      "3. **Create Custom Dashboards**: Leverage out-of-the-box dashboards provided by the monitoring tool to gain insights into your applications. Customize these dashboards to display metrics that are critical to your monitoring objectives.\n",
      "\n",
      "4. **Set Up Alerts**: Configure alerts based on thresholds for key metrics to proactively identify and address issues. This ensures that you are promptly notified of any deviations from expected performance levels.\n",
      "\n",
      "5. **Utilize Clustering and Checks**: Use features like topical clustering, sentiment analysis, failure to answer checks, and other tools available in the monitoring solution to identify problematic areas in your LLM applications.\n",
      "\n",
      "6. **Regularly Review and Analyze Data**: Schedule regular reviews of the monitoring data to identify trends, patterns, and areas for improvement. Use this analysis to optimize the performance and cost-effectiveness of your LLM applications.\n",
      "\n",
      "7. **Collaborate Across Teams**: Share monitoring insights with relevant teams such as development, operations, and business stakeholders to drive collaborative efforts in optimizing LLM applications.\n",
      "\n",
      "8. **Continuously Improve**: Implement a feedback loop based on monitoring data to continuously improve the quality and effectiveness of your LLM-powered applications. Use data-driven insights to make informed decisions on optimizations.\n",
      "\n",
      "By following these best practices, you can effectively set up and utilize a SaaS monitoring tool like LLM Observability to enhance the monitoring and analysis of your LLM applications.\n",
      "Question 5: \"What are the key steps involved in setting up and effectively utilizing a SaaS tool to monitor and analyze LLM-powered applications?\"\n",
      "Answer 5: Setting up and effectively utilizing a SaaS tool like LLM Observability by Datadog to monitor and analyze LLM-powered applications involves several key steps:\n",
      "\n",
      "1. **Integration and Configuration**: Connect your LLM application to the monitoring tool by integrating the necessary plugins or agents. Configure the tool to collect data from your application, including logs, metrics, and traces.\n",
      "\n",
      "2. **Define Key Metrics**: Identify the key performance indicators (KPIs) that are crucial for monitoring the health and performance of your LLM applications. These could include response times, error rates, throughput, etc.\n",
      "\n",
      "3. **Create Dashboards**: Utilize out-of-the-box dashboards provided by the monitoring tool to visualize the performance of your LLM applications. Customize these dashboards to display the specific metrics that matter most to your team.\n",
      "\n",
      "4. **Set Alerts**: Define thresholds for key metrics and set up alerts to notify you when these thresholds are breached. This proactive monitoring helps you address issues before they impact your users.\n",
      "\n",
      "5. **Analyze Data**: Regularly review the data collected by the monitoring tool to identify trends, anomalies, or potential issues in your LLM applications. Use tools like topical clustering, sentiment analysis, failure analysis, etc., to gain deeper insights.\n",
      "\n",
      "6. **Optimize Performance**: Use the data collected by the monitoring tool to optimize the performance of your LLM applications. Identify bottlenecks, optimize resource usage, and improve response times based on the insights gathered.\n",
      "\n",
      "7. **Continuous Improvement**: Monitoring and analyzing LLM applications is an ongoing process. Regularly review your monitoring setup, adjust alerts and dashboards as needed, and constantly strive to improve the performance and reliability of your applications.\n",
      "\n",
      "By following these key steps, you can effectively set up and utilize a SaaS tool like LLM Observability to monitor and analyze your LLM-powered applications, ensuring their quality and effectiveness over time.\n",
      "Question 6: What are the best practices for setting up and effectively utilizing a SaaS tool to monitor and analyze LLM-powered applications?\n",
      "Answer 6: Setting up and effectively utilizing a SaaS tool like LLM Observability by Datadog to monitor and analyze LLM-powered applications involves following some best practices. Here are some recommendations:\n",
      "\n",
      "1. **Define Monitoring Goals**: Clearly define the key metrics and goals you want to monitor for your LLM applications. This can include cost optimization, latency, performance, usage trends, response quality, sentiment analysis, etc.\n",
      "\n",
      "2. **Instrumentation**: Properly instrument your LLM applications to collect relevant metrics and logs. Ensure that your application's logs are structured and include important information for analysis.\n",
      "\n",
      "3. **Utilize Out-of-the-Box Dashboards**: Take advantage of the out-of-the-box dashboards provided by LLM Observability to monitor operational metrics like cost, latency, performance, and usage trends. These dashboards can provide valuable insights into the health of your applications.\n",
      "\n",
      "4. **Customize Dashboards**: Customize dashboards to track specific metrics and KPIs that are important to your business. This can help you focus on what matters most for your LLM applications.\n",
      "\n",
      "5. **Set Up Alerts**: Configure alerts based on thresholds for critical metrics to proactively identify issues and take action before they affect your application's performance or cost.\n",
      "\n",
      "6. **Utilize Clustering and Checks**: Make use of the clustering functionality in LLM Observability to identify problematic clusters and monitor the quality of responses over time. This can help you detect patterns and trends that require attention.\n",
      "\n",
      "7. **Regularly Review and Analyze Data**: Regularly review the monitoring data and analyze trends to identify areas for improvement or optimization. Use this data to make informed decisions about your LLM applications.\n",
      "\n",
      "8. **Collaborate Across Teams**: Involve different teams (such as development, operations, and business) in the monitoring and analysis process to ensure visibility and alignment on goals and actions.\n",
      "\n",
      "By following these best practices, you can effectively set up and utilize a SaaS tool like LLM Observability to monitor and analyze your LLM-powered applications, optimize performance, and make data-driven decisions.\n",
      "Question 7: How can you effectively set up and utilize a SaaS tool to monitor and analyze the performance of LLM-powered applications?\n",
      "Answer 7: To effectively set up and utilize a SaaS tool like LLM Observability by Datadog to monitor and analyze the performance of LLM-powered applications, you can follow these steps:\n",
      "\n",
      "1. **Integration**: Integrate the LLM Observability tool with your LLM-powered applications. This typically involves installing an agent or SDK in your application code. Datadog provides documentation and guides on how to perform this integration.\n",
      "\n",
      "2. **Define Key Metrics**: Determine which key metrics are important for monitoring the performance of your LLM applications. These may include latency, response times, failure rates, throughput, and resource utilization.\n",
      "\n",
      "3. **Set Up Dashboards**: Use the out-of-the-box dashboards provided by LLM Observability to monitor operational metrics, cost, latency, performance, and usage trends for all your LLM applications. Customize these dashboards to display the specific metrics and visualizations that are most relevant to your needs.\n",
      "\n",
      "4. **Monitor Clusters**: Utilize the clustering feature in LLM Observability to identify problematic clusters within your LLM applications. Monitor the quality of responses over time using checks like sentiment analysis and failure to answer metrics.\n",
      "\n",
      "5. **Alerting**: Set up alerting rules in LLM Observability to receive notifications when performance metrics exceed predefined thresholds. This allows you to proactively address issues before they impact end users.\n",
      "\n",
      "6. **Analyze Trends**: Use the data collected by LLM Observability to analyze trends in the performance of your LLM applications over time. Identify patterns, anomalies, and areas for optimization.\n",
      "\n",
      "7. **Optimize Cost**: Monitor the cost of running your LLM applications and identify opportunities to optimize resource usage and reduce expenses. Use cost optimization features in LLM Observability to make data-driven decisions.\n",
      "\n",
      "By following these steps, you can effectively set up and utilize a SaaS tool like LLM Observability to monitor and analyze the performance of your LLM-powered applications, helping you ensure optimal performance and user experience.\n",
      "Question 8: What are some best practices for setting up and effectively using a SaaS tool to monitor and analyze the performance of LLM-powered applications?\n",
      "Answer 8: When setting up and using a SaaS tool like LLM Observability by Datadog to monitor and analyze the performance of LLM-powered applications, here are some best practices to follow:\n",
      "\n",
      "1. **Define Key Metrics**: Identify the key performance indicators (KPIs) specific to your LLM applications. This could include latency, cost, performance, usage trends, sentiment analysis, failure rates, etc.\n",
      "\n",
      "2. **Set up Alerts**: Configure alerts based on thresholds for these key metrics. This allows you to be notified in real-time of any issues that may impact the performance of your LLM applications.\n",
      "\n",
      "3. **Utilize Out-of-the-Box Dashboards**: Take advantage of pre-built dashboards provided by LLM Observability like the Operational Insights dashboard to monitor cost, latency, performance, and usage trends. These dashboards provide a quick overview of your application's health.\n",
      "\n",
      "4. **Customize Dashboards**: Tailor dashboards to display metrics that are most relevant to your specific use case. Add and arrange widgets to track the metrics that matter most to you.\n",
      "\n",
      "5. **Utilize Topical Clustering**: Use features like topical clustering to identify problematic clusters within your LLM applications. Monitor the quality of responses over time and track metrics like sentiment analysis and failure rates.\n",
      "\n",
      "6. **Regularly Review and Analyze Data**: Schedule regular reviews of the data collected by the monitoring tool. Look for trends, anomalies, and areas of improvement within your LLM applications.\n",
      "\n",
      "7. **Optimize Costs**: Keep track of cost metrics and optimize resource allocation to reduce unnecessary expenses. Monitor cost trends over time and take actions to economize where possible.\n",
      "\n",
      "8. **Collaborate and Share Insights**: Share insights and dashboards with relevant team members to foster collaboration and keep everyone informed about the performance of the LLM applications.\n",
      "\n",
      "9. **Continuously Improve**: Use the data and insights gathered to continuously improve the performance and quality of your LLM applications. Implement changes based on the findings to enhance user experience and efficiency.\n",
      "\n",
      "By following these best practices, you can effectively set up and utilize a SaaS monitoring tool like LLM Observability to ensure the optimal performance of your LLM-powered applications.\n",
      "Question 9: How can a user effectively set up and utilize a SaaS tool to monitor and optimize LLM-powered applications?\n",
      "Answer 9: To effectively set up and utilize a SaaS tool like LLM Observability by Datadog to monitor and optimize LLM-powered applications, follow these steps:\n",
      "\n",
      "1. **Onboarding and Setup**:\n",
      "   - Sign up for the LLM Observability service provided by Datadog.\n",
      "   - Install any required agents or integrations to connect your LLM-powered applications to the monitoring tool.\n",
      "   - Configure the tool to collect relevant metrics and logs from your applications.\n",
      "\n",
      "2. **Dashboard Setup**:\n",
      "   - Explore the out-of-the-box dashboards provided by LLM Observability to monitor cost, latency, performance, and usage trends.\n",
      "   - Customize dashboards to track specific metrics and key performance indicators (KPIs) for your LLM applications.\n",
      "\n",
      "3. **Set Alerts and Notifications**:\n",
      "   - Define alert conditions based on thresholds or anomalies in your metrics.\n",
      "   - Configure notifications to be sent via email, SMS, or other channels when an alert is triggered.\n",
      "\n",
      "4. **Performance Optimization**:\n",
      "   - Analyze the data collected by the monitoring tool to identify performance bottlenecks in your LLM-powered applications.\n",
      "   - Use the insights from the tool to optimize resource allocation, improve response times, and enhance overall application performance.\n",
      "\n",
      "5. **Cost Optimization**:\n",
      "   - Monitor the cost metrics provided by the tool to identify opportunities for cost savings.\n",
      "   - Implement cost optimization strategies, such as scaling resources based on demand or leveraging cost-effective services.\n",
      "\n",
      "6. **Quality Assessment**:\n",
      "   - Utilize features like topical clustering and sentiment analysis to evaluate the quality of responses generated by your LLM applications.\n",
      "   - Monitor changes in response quality over time and identify any issues that need to be addressed.\n",
      "\n",
      "7. **Continuous Improvement**:\n",
      "   - Regularly review the monitoring data and insights provided by the tool to identify areas for improvement.\n",
      "   - Implement changes based on the recommendations and best practices suggested by the monitoring tool.\n",
      "\n",
      "By following these steps and leveraging the features and capabilities of LLM Observability by Datadog, users can effectively monitor and optimize their LLM-powered applications to ensure optimal performance, cost efficiency, and quality of responses.\n",
      "Question 10: How can I effectively setup and optimize the use of a SaaS monitoring tool to observe the performance of LLM-powered applications?\n",
      "Answer 10: To effectively set up and optimize the use of a SaaS monitoring tool like LLM Observability by Datadog to observe the performance of LLM-powered applications, follow these steps:\n",
      "\n",
      "1. **Integration Setup**: Integrate your LLM applications with the monitoring tool to start collecting data. Most monitoring tools provide agents or APIs that you can use to send data from your applications to the tool.\n",
      "\n",
      "2. **Define Key Metrics**: Determine the key performance indicators (KPIs) that are important for your LLM applications. This could include latency, response time, error rates, throughput, resource utilization, and more.\n",
      "\n",
      "3. **Set Up Dashboards**: Leverage out-of-the-box dashboards provided by the monitoring tool to visualize the performance metrics of your LLM applications. Customize these dashboards to display the specific metrics that are relevant to your use case.\n",
      "\n",
      "4. **Alerting Configuration**: Configure alerts based on predefined thresholds for your key metrics. This way, you will be notified in real-time if there are any performance issues with your LLM applications.\n",
      "\n",
      "5. **Anomaly Detection**: Use anomaly detection features provided by the monitoring tool to automatically detect unusual behavior in your applications. This can help you proactively address issues before they impact users.\n",
      "\n",
      "6. **Cost Optimization**: Monitor the cost of running your LLM applications to ensure efficient resource utilization. Look for ways to optimize costs without compromising performance.\n",
      "\n",
      "7. **Evaluate Clusters and Responses**: Use features like topical clustering and sentiment analysis to identify problematic clusters in your LLM applications. Monitor the quality of responses over time to ensure a good user experience.\n",
      "\n",
      "8. **Regular Review and Optimization**: Regularly review the performance metrics, alerts, and feedback from the monitoring tool to identify areas for optimization. Continuously iterate on your monitoring setup to improve the performance of your LLM applications.\n",
      "\n",
      "By following these steps, you can effectively set up and optimize the use of a SaaS monitoring tool like LLM Observability to observe the performance of your LLM-powered applications.\n",
      "Question 11: How can I effectively set up and maximize the use of a SaaS tool to monitor and optimize LLM-powered applications in real-time?\n",
      "Answer 11: To effectively set up and maximize the use of a SaaS tool like LLM Observability by Datadog to monitor and optimize LLM-powered applications in real-time, you can follow these steps:\n",
      "\n",
      "1. **Integration and Configuration**:\n",
      "   - Integrate the LLM Observability tool with your LLM-powered applications by following the setup instructions provided.\n",
      "   - Configure the tool to collect data from your applications, including metrics like cost, latency, performance, and usage.\n",
      "\n",
      "2. **Dashboard Setup**:\n",
      "   - Utilize the out-of-the-box dashboards provided by LLM Observability to monitor operational metrics and trends for your applications.\n",
      "   - Customize the dashboards to focus on the specific metrics that are critical for your use case.\n",
      "\n",
      "3. **Cluster Monitoring**:\n",
      "   - Use the clustering feature to identify problematic clusters within your applications.\n",
      "   - Monitor the quality of responses over time using checks like sentiment analysis, failure to answer, and others available in the tool.\n",
      "\n",
      "4. **Alerting**:\n",
      "   - Set up alerts based on thresholds for key metrics to be notified in real-time of any issues or anomalies in your applications.\n",
      "   - Configure alerts to be sent to relevant team members for prompt action.\n",
      "\n",
      "5. **Cost Optimization**:\n",
      "   - Monitor the cost metrics of your LLM applications to identify areas where optimization is possible.\n",
      "   - Use the insights provided by the tool to make informed decisions on optimizing costs without compromising performance.\n",
      "\n",
      "6. **Continuous Optimization**:\n",
      "   - Regularly review the data and insights provided by the tool to identify areas for improvement in your LLM applications.\n",
      "   - Implement changes based on the recommendations to continuously optimize the performance and efficiency of your applications.\n",
      "\n",
      "7. **Training and Support**:\n",
      "   - Ensure that your team is trained on how to use the LLM Observability tool effectively.\n",
      "   - Reach out to the support team provided by the tool for any assistance or guidance in maximizing its use for monitoring and optimizing your LLM applications.\n",
      "\n",
      "By following these steps, you can effectively set up and maximize the use of a SaaS tool like LLM Observability to monitor and optimize your LLM-powered applications in real-time.\n",
      "Question 12: How can I effectively set up and utilize a SaaS tool to monitor and analyze LLM-powered applications for optimal performance and efficiency?\n",
      "Answer 12: To effectively set up and utilize the LLM Observability by Datadog SaaS tool to monitor and analyze LLM-powered applications for optimal performance and efficiency, you can follow these steps:\n",
      "\n",
      "1. **Integration**: Integrate your LLM application with LLM Observability by Datadog by following the provided integration instructions. This will allow you to start collecting data and metrics from your application.\n",
      "\n",
      "2. **Dashboard Setup**: Take advantage of the out-of-the-box dashboards provided by LLM Observability to monitor operational metrics, cost, latency, performance, and usage trends for all your LLM applications. These dashboards will give you a high-level overview of the performance of your applications.\n",
      "\n",
      "3. **Custom Dashboards**: Create custom dashboards as needed to focus on specific metrics or aspects of your LLM-powered applications that are important to you. For example, you can create dashboards to monitor sentiment analysis, failure rates, or response quality over time.\n",
      "\n",
      "4. **Alerting**: Set up alerts based on predefined thresholds or anomalies in your metrics. This will allow you to proactively identify and address issues before they impact your application's performance.\n",
      "\n",
      "5. **Clusters Analysis**: Utilize the clusters page in LLM Observability to identify problematic clusters within your applications. Monitor the quality of responses over time using features like topical clustering, sentiment analysis, and failure to answer checks.\n",
      "\n",
      "6. **Optimization**: Use the insights gathered from monitoring and analysis to optimize the performance and efficiency of your LLM-powered applications. Identify areas for improvement and take action to enhance the overall quality of your applications.\n",
      "\n",
      "By following these steps and leveraging the features provided by LLM Observability by Datadog, you can effectively monitor and analyze your LLM-powered applications to ensure optimal performance and efficiency.\n",
      "Question 13: How can I effectively set up and utilize a SaaS tool to monitor and analyze LLM-powered applications for optimum performance and efficiency?\n",
      "Answer 13: To effectively set up and utilize a SaaS tool like LLM Observability by Datadog for monitoring and analyzing LLM-powered applications, you can follow these steps:\n",
      "\n",
      "1. **Onboarding and Configuration**:\n",
      "   - Begin by setting up an account with LLM Observability by Datadog and installing any necessary agents or integrations for your LLM-powered applications.\n",
      "   - Configure the tool to collect relevant operational metrics, such as cost, latency, performance, and usage data from your applications.\n",
      "\n",
      "2. **Dashboard Setup**:\n",
      "   - Utilize the out-of-the-box dashboards provided by LLM Observability to monitor key metrics and trends related to your applications. These dashboards can help you quickly visualize and understand the performance and efficiency of your LLM applications.\n",
      "\n",
      "3. **Custom Dashboards**:\n",
      "   - Create custom dashboards tailored to your specific monitoring needs. You can add widgets and visualizations to track important KPIs, such as sentiment analysis, response quality, failure rates, and more.\n",
      "\n",
      "4. **Alerting**:\n",
      "   - Set up alerts based on thresholds and conditions that indicate potential issues or optimization opportunities in your LLM applications. This will help you proactively address issues before they impact performance.\n",
      "\n",
      "5. **Cluster Analysis**:\n",
      "   - Utilize the clustering feature in LLM Observability to identify problematic clusters within your applications. Monitor the quality of responses over time, and use this information to optimize performance and efficiency.\n",
      "\n",
      "6. **Cost Optimization**:\n",
      "   - Monitor the cost trends of your LLM applications using the available tools. Identify areas where cost optimization is possible and take action to ensure efficient resource allocation.\n",
      "\n",
      "7. **Regular Review and Optimization**:\n",
      "   - Regularly review the data and insights provided by LLM Observability to identify areas for optimization and improvement in your LLM-powered applications. Use this information to make informed decisions and enhance the overall performance and efficiency of your applications.\n",
      "\n",
      "By following these steps and leveraging the features of LLM Observability by Datadog, you can effectively monitor and analyze your LLM-powered applications for optimum performance and efficiency.\n",
      "Question 14: What are the key steps to properly set up and effectively utilize a SaaS tool for observing LLM-powered applications?\n",
      "Answer 14: Setting up and effectively utilizing a SaaS tool like LLM Observability by Datadog for observing LLM-powered applications involves the following key steps:\n",
      "\n",
      "1. **Installation and Configuration**: \n",
      "   - Install the LLM Observability tool provided by Datadog and configure it to monitor your LLM applications.\n",
      "   - Ensure that the tool is correctly integrated with your LLM application ecosystem.\n",
      "\n",
      "2. **Define Key Metrics**:\n",
      "   - Identify the key metrics and KPIs that are important for monitoring the health and performance of your LLM applications.\n",
      "   - These metrics can include response time, error rate, throughput, and any other relevant performance indicators.\n",
      "\n",
      "3. **Set Up Alerts**:\n",
      "   - Configure alerts based on the defined metrics to be notified of any anomalies or issues in real-time.\n",
      "   - Set thresholds for these alerts to trigger notifications when performance deviates from expected norms.\n",
      "\n",
      "4. **Monitor Clusters**:\n",
      "   - Utilize the clustering feature to identify problematic clusters within your LLM applications.\n",
      "   - Monitor the quality of responses over time and use checks like sentiment analysis, failure to answer, etc., to improve application performance.\n",
      "\n",
      "5. **Optimize Cost and Performance**:\n",
      "   - Utilize the out-of-the-box dashboards provided by LLM Observability to monitor cost, latency, performance, and usage trends for all your LLM applications.\n",
      "   - Use this data to optimize costs, improve performance, and ensure efficient resource allocation.\n",
      "\n",
      "6. **Regularly Review and Analyze Data**:\n",
      "   - Regularly review and analyze the data collected by the monitoring tool.\n",
      "   - Look for trends, patterns, and areas of improvement to continuously enhance the performance of your LLM applications.\n",
      "\n",
      "7. **Collaborate and Iterate**:\n",
      "   - Collaborate with your team members, developers, and stakeholders to share insights and work on improvements.\n",
      "   - Iterate on your monitoring strategy based on feedback, new requirements, and changing application needs.\n",
      "\n",
      "By following these key steps, you can effectively set up and utilize a SaaS tool like LLM Observability to monitor and optimize your LLM-powered applications for better performance and reliability.\n",
      "Question 15: \"How can teams effectively set up and optimize the use of a SaaS tool to monitor and analyze the performance of LLM-powered applications?\"\n",
      "Answer 15: Setting up and optimizing the use of the LLM Observability tool by Datadog to monitor and analyze the performance of your LLM-powered applications involves several key steps:\n",
      "\n",
      "1. **Onboarding and Setup:**\n",
      "   - Ensure that all relevant team members are onboarded to the tool and have the necessary permissions.\n",
      "   - Connect your LLM applications to the monitoring tool to start collecting data.\n",
      "\n",
      "2. **Define Key Metrics:**\n",
      "   - Identify the most critical metrics for monitoring the performance of your LLM applications, such as latency, response times, error rates, etc.\n",
      "\n",
      "3. **Set Up Dashboards:**\n",
      "   - Utilize the out-of-the-box dashboards provided by LLM Observability to monitor cost, latency, performance, and usage trends.\n",
      "   - Customize the dashboards to display the specific metrics and KPIs that are most relevant to your team.\n",
      "\n",
      "4. **Alerting and Notifications:**\n",
      "   - Configure alerts to notify your team of any issues or anomalies in real-time.\n",
      "   - Set up thresholds for key metrics to trigger alerts when they exceed predefined limits.\n",
      "\n",
      "5. **Cluster Monitoring:**\n",
      "   - Utilize the cluster monitoring feature to identify problematic clusters and monitor the quality of responses over time.\n",
      "   - Use topical clustering and checks like sentiment analysis, failure to answer, etc., to gain deeper insights into the performance of your applications.\n",
      "\n",
      "6. **Continuous Optimization:**\n",
      "   - Regularly review and analyze the data collected by the monitoring tool to identify areas for improvement.\n",
      "   - Use the insights gained from the tool to optimize the performance and cost-effectiveness of your LLM applications.\n",
      "\n",
      "7. **Collaboration and Knowledge Sharing:**\n",
      "   - Encourage collaboration among team members by sharing dashboards and reports generated by the monitoring tool.\n",
      "   - Use the data and insights from the tool to facilitate discussions and decision-making within the team.\n",
      "\n",
      "By following these steps and leveraging the features of LLM Observability by Datadog, teams can effectively set up and optimize the use of a SaaS tool to monitor and analyze the performance of their LLM-powered applications.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m answer\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m     question \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m     answer \u001b[38;5;241m=\u001b[39m ask_docs(question)\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mgenerate_question\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_question\u001b[39m():\n\u001b[1;32m      2\u001b[0m     answer \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      3\u001b[0m         \u001b[43moai_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenerate a question about how to setup & best use a SaaS tool to observe LLM-powered applications\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m---> 11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'choices'"
     ]
    }
   ],
   "source": [
    "def generate_question():\n",
    "    answer = (\n",
    "        oai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"generate a question about how to setup & best use a SaaS tool to observe LLM-powered applications\",\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "    return answer\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    question = generate_question()\n",
    "    print(f\"Question {i+1}: {question}\")\n",
    "    answer = ask_docs(question)\n",
    "    print(f\"Answer {i+1}: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
