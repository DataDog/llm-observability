{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup\n",
    "\n",
    "The following cells install dependencies and setup LLM tracing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load required env vars for ddtrace\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ddtrace.auto\n",
    "\n",
    "ddtrace.patch_all()\n",
    "\n",
    "from ddtrace.llmobs import LLMObs\n",
    "\n",
    "LLMObs.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracing a Workflow Span\n",
    "\n",
    "In this notebook, we are building a service that takes a free text query about art from a user, and feeds it into the Metropolitan Museum of Art API to get a list of artwork.\n",
    "\n",
    "The steps are:\n",
    "\n",
    "1. Take a query from a user\n",
    "2. Parse that query via a call to OpenAI\n",
    "3. Send the parsed query to the [Metropolitan Museum of Art API](https://metmuseum.github.io/#search)\n",
    "4. Return a list of urls to the user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Creating the tool to fetch data from the Met API**\n",
    "\n",
    "In the next cell, we create and instrument a \"tool\" to send a query to the MET API's `/search` endpoint. The query will be created by a LLM call in a following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from ddtrace.llmobs.decorators import tool\n",
    "import json\n",
    "\n",
    "SEARCH_ENDPOINT = \"https://collectionapi.metmuseum.org/public/collection/v1/search\"\n",
    "MAX_RESULTS = 5\n",
    "\n",
    "\n",
    "@tool(name=\"fetch_met_urls\")\n",
    "def fetch_met_urls(query_parameters):\n",
    "    LLMObs.annotate(\n",
    "        input_data=json.dumps(query_parameters),\n",
    "    )\n",
    "    response = requests.get(SEARCH_ENDPOINT, params=query_parameters)\n",
    "    response.raise_for_status()\n",
    "    object_ids = response.json().get(\"objectIDs\")\n",
    "    objects_to_return = object_ids[:MAX_RESULTS] if object_ids else []\n",
    "    urls = [\n",
    "        f\"https://www.metmuseum.org/art/collection/search/{objectId}\"\n",
    "        for objectId in objects_to_return\n",
    "    ]\n",
    "    LLMObs.annotate(\n",
    "        output_data=json.dumps(urls),\n",
    "    )\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Creating an LLM call to handle parsing user input into a standardized query**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from ddtrace.llmobs.decorators import workflow\n",
    "from ddtrace.llmobs import LLMObs\n",
    "\n",
    "\n",
    "oai_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# we have scraped the Met API docs (https://metmuseum.github.io/#search)\n",
    "# and saved the parameters in a json structure that we can inline into the prompt:\n",
    "with open(\"met-search-endpoint-parameters.json\", \"r\") as f:\n",
    "    parameter_info = json.load(f)\n",
    "\n",
    "    sys_prompt = f\"\"\"\n",
    "You are a service that provides information about the art in the collection of the Metropolitan Museum of Art.\n",
    "You take a free text query and parse it into a format that can be sent to the Met API's search endpoint.\n",
    "\n",
    "Here is some information about the parameters accepted by the search endpoint:\n",
    "\n",
    "{parameter_info}\n",
    "\n",
    "The `hasImages` parameter must always be set to `true`.\n",
    "\n",
    "Parse a query out from the user's message in a JSON format. Example:\n",
    "\n",
    "Prompt: I'd like to see some highlights from the Met collection. Can you show me some paintings with flowers or other still lifes from around the 1800s?\n",
    "\n",
    "Response: {{\n",
    "    \"q\": \"flowers still life\",\n",
    "    \"hasImages\": \"true\",\n",
    "    \"isHighlight\": \"true\",\n",
    "    \"dateBegin\": \"1800\",\n",
    "    \"dateEnd\": \"1899\",\n",
    "    \"medium\": \"Paintings\"\n",
    "}}\n",
    " \"\"\"\n",
    "\n",
    "\n",
    "def parse_query(message, prompt=sys_prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    response = (\n",
    "        oai_client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "    query_parameters = json.loads(response)\n",
    "    print(f\"Parsed query parameters: {query_parameters}\")\n",
    "    return query_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Creating the `find_artworks` function**\n",
    "\n",
    "Finally, we are creating a `find_artworks` function here that will tie the LLM call and tool call together. We annotate this as a workflow span:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@workflow(name=\"find_artworks\")\n",
    "def find_artworks(question):\n",
    "    LLMObs.annotate(\n",
    "        input_data=question,\n",
    "    )\n",
    "    query = parse_query(question)\n",
    "    urls = fetch_met_urls(query)\n",
    "    LLMObs.annotate(\n",
    "        output_data=json.dumps(urls),\n",
    "    )\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = find_artworks(\"I'd like to see some art from Ancient Greece.\")\n",
    "print(\"returned urls:\", urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
