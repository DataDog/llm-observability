{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting Custom Evaluations for your LLM Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datadog's LLM Observability tool allows you to perform evaluations on your LLM application and tie these evaluations to specific traces. \n",
    "\n",
    "In this notebook, we'll build a simple LLM application involving user feedback. We'll generate dummy feedback data and submit them to LLM Observability.\n",
    "\n",
    "#### Learning Goals\n",
    "- Understand how to export span context to correlate with your custom evaluation\n",
    "- Understand how to submit custom evaluations tied to specific traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "import time\n",
    "\n",
    "from ddtrace.llmobs import LLMObs\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "LLMObs.enable(\n",
    "    api_key=os.environ.get(\"DD_API_KEY\"),\n",
    "    site=os.environ.get(\"DD_SITE\", \"datadoghq.com\"),\n",
    "    ml_app=\"ask-llmobs-docs\",\n",
    "    agentless_enabled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample LLM Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrumenting the Application\n",
    "\n",
    "Here's a sample LLM application workflow involving 2 steps:\n",
    "1. Preprocessing to sanitize user input\n",
    "2. Send the sanitized user input to an LLM call to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(prompt):\n",
    "    client = openai.OpenAI()\n",
    "    resp = client.completions.create(model=\"gpt-3.5-turbo-instruct\", prompt=prompt, temperature=0.9, max_tokens=50)\n",
    "    return resp.choices[0].text\n",
    "\n",
    "def sanitize_prompt(prompt):\n",
    "    time.sleep(0.3)  # to simulate more complex work\n",
    "    sanitized_prompt = prompt.strip(\"invalid_string\").strip()\n",
    "    return sanitized_prompt\n",
    "\n",
    "\n",
    "def workflow_trace(prompt):\n",
    "    sanitized_prompt = sanitize_prompt(prompt)\n",
    "    resp = llm_call(sanitized_prompt)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instrument `sanitize_prompt()` and `workflow_trace()`, since the OpenAI call is auto-instrumented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddtrace.llmobs.decorators import task, workflow\n",
    "\n",
    "@task\n",
    "def sanitize_prompt(prompt):\n",
    "    time.sleep(0.3)  # to simulate more complex work\n",
    "    sanitized_prompt = prompt.strip(\"invalid_string\").strip()\n",
    "    LLMObs.annotate(input_data=prompt, output_data=sanitized_prompt)\n",
    "    return sanitized_prompt\n",
    "\n",
    "@workflow\n",
    "def workflow_trace(prompt):\n",
    "    sanitized_prompt = sanitize_prompt(prompt)\n",
    "    resp = llm_call(sanitized_prompt)\n",
    "    LLMObs.annotate(input_data=prompt, output_data=resp)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this application instrumented, feel free to run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The greatest basketball player of all time is widely considered to be Michael Jordan. There are several reasons why he is considered the greatest:\n",
      "\n",
      "1. Unmatched Accomplishments: Jordan won 6 NBA Championships, 5 MVP awards, and 10\n"
     ]
    }
   ],
   "source": [
    "print(workflow_trace(\"invaid_str                                Who is the greatest basketball player of all time and why?              invalid_str\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Evaluations\n",
    "\n",
    "Now that we have the application instrumented, we can submit traces to LLM Observability. However, here's a user feedback recording helper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _measure_user_satisfaction(resp):\n",
    "    \"\"\"Dummy feedback generator, recording user satisfaction with the response from a scale of 1 to 10.\"\"\"\n",
    "    return 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can submit this custom evaluation to LLM Observability by correlating it with the trace from before. We just need to add a few tweaks to:\n",
    "- Export the span context of `workflow_trace()` using `LLMObs.export_span()` and save that for later.\n",
    "- Pass that exported span context from `workflow_trace()` to `record_user_satisfaction()`, and use `LLMObs.submit_evaluation()` to submit the evaluation to Datadog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@workflow\n",
    "def workflow_trace(prompt):\n",
    "    sanitized_prompt = sanitize_prompt(prompt)\n",
    "    resp = llm_call(sanitized_prompt)\n",
    "    LLMObs.annotate(input_data=prompt, output_data=resp)\n",
    "    # Export the span here and return it as well as the response\n",
    "    return resp\n",
    "\n",
    "def record_user_satisfaction(resp, span_context):\n",
    "    satisfaction_value = _measure_user_satisfaction(resp)\n",
    "    # Submit the evaluation to Datadog using the exported span context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've done that, your code should look something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@workflow\n",
    "def workflow_trace(prompt):\n",
    "    sanitized_prompt = sanitize_prompt(prompt)\n",
    "    resp = llm_call(sanitized_prompt)\n",
    "    LLMObs.annotate(input_data=prompt, output_data=resp)\n",
    "    # Export the span here and return it as well as the response\n",
    "    span_context = LLMObs.export_span()\n",
    "    return resp, span_context\n",
    "\n",
    "def record_user_satisfaction(resp, span_context):\n",
    "    satisfaction_value = _measure_user_satisfaction(resp)\n",
    "    # Submit the evaluation to Datadog using the exported span context\n",
    "    LLMObs.submit_evaluation(span_context, label=\"user_satisfaction\", metric_type=\"score\", value=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your LLM workflow and recording the user satisfaction method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp, span_context = workflow_trace(\"invaid_str                                Who is the greatest soccer player of all time and why?             invalid_str\")\n",
    "print(resp)\n",
    "record_user_satisfaction(resp, span_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try checking out the [LLM Observability interface](https://app.datadoghq.com/llm) in Datadog. You should see a trace that describes the workflow we just ran, and you should see the custom evaluation we've associated with this trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
