{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running an experiment 🧪\n",
    "\n",
    "In this notebook, we'll walk through how to run a basic experiment.\n",
    "\n",
    "An experiment consists of three key components: a dataset, a task, and evaluators.\n",
    "- **Task**: A function that takes an input and generates a response.\n",
    "- **Evaluators**: Functions that compare the model's output against the expected output (along with the input) and returns a value based on specific criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's initialize the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file.\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import ddtrace.llmobs.experimentation as dne\n",
    "\n",
    "dne.init(project_name=\"Onboarding\")\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will reuse the `capitals-of-the-world` dataset we created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the dataset from Datadog\n",
    "dataset = dne.Dataset.pull(name=\"capitals-of-the-world\")\n",
    "dataset.as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task definition\n",
    "\n",
    "This is the core of the experiment.\n",
    "\n",
    "We will first define the task we want to evaluate and will decorate it using `@dne.task`.\n",
    "\n",
    "You can evaluate from single prompts to complex agents inside this function. This function will have access to one dataset row, and it will be executed row-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function must accept an `input` parameter, which represents a single dataset row.\n",
    "@dne.task\n",
    "def basic_generate_capital_name(input):\n",
    "    # Inside this function, you can perform any logic you want. In this case, we'll use the OpenAI API to generate a response given the input.\n",
    "    output = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": input}],\n",
    "        temperature=1.4\n",
    "    )\n",
    "    return output.choices[0].message.content\n",
    "\n",
    "# We now define the experiment with a descriptive name, dataset, task function, and evaluators.\n",
    "experiment = dne.Experiment(\n",
    "    name=\"gpt-4o-mini\", # <---- We can use any name we want for the experiment.\n",
    "    dataset=dataset,\n",
    "    task=basic_generate_capital_name,\n",
    "    evaluators=[]  # Empty for now; we’ll add evaluators later\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can run the experiment! This will execute the task against the dataset in a row-wise manner concurrently so that it runs faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = experiment.run(jobs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can access the results locally if we want.\n",
    "results.as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running evaluations 🔬\n",
    "\n",
    "Evaluators are functions that assess the model’s performance by comparing the expected output with the actual output generated by the model. They receive the following parameters:\n",
    "- input – The original input prompt.\n",
    "- output – The model's generated response.\n",
    "- expected_output – The correct answer from the dataset.\n",
    "\n",
    "The evaluator should return a score or assessment based on a defined criterion.\n",
    "\n",
    "#### Example: Exact Match Evaluator\n",
    "For this experiment, we create a simple equality check to determine whether the model’s response matches the expected answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom evaluator to check if the model's output matches the expected answer.\n",
    "# Evaluators receive `input`, `output` (model-generated), and `expected_output` (ground truth). All of them come automatically from the dataset and the task.\n",
    "# You can modify the logic to support different evaluation methods like fuzzy matching, semantic similarity, llm-as-a-judge, etc.\n",
    "@dne.evaluator\n",
    "def exact_match(input, output, expected_output):\n",
    "    return expected_output == output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = dne.Experiment(\n",
    "    name=\"gpt-4o-mini-with-evals\", \n",
    "    dataset=dataset, \n",
    "    task=basic_generate_capital_name,\n",
    "    evaluators=[exact_match],\n",
    ")\n",
    "\n",
    "results = experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Another Evaluator 🔍\n",
    "The exact match evaluator may not always work well, especially when the model responds in a conversational format. Even if the answer is correct, slight variations in phrasing can cause mismatches.\n",
    "\n",
    "To address this, we'll add an evaluator that checks whether the expected answer appears within the model's output, allowing for more flexible matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dne.evaluator\n",
    "def contains_answer(input, output, expected_output):\n",
    "    return expected_output in output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the new evaluator, there's no need to rerun the entire experiment. Instead, we can re-evaluate the existing results using the newly added evaluator.\n",
    "\n",
    "This allows us to efficiently test different evaluation strategies without incurring the computational cost of running the experiment again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = experiment.run_evaluations([contains_answer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see now two new fields, `exact_match` and `contains_answer`, in the `evaluations` column in the Datadog UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before finishing, let's talk about configurations\n",
    "\n",
    "`config` is a dictionary that can be used to parametrize the experiment. It can be used to change the model, the temperature, the system prompt, and other parameters.\n",
    "\n",
    "**Note:** The `config` parameter is optional. It's a way to record the configuration on each experiment for later analysis. This is useful later on in the UI to compare different configurations.\n",
    "\n",
    "\n",
    "The `config` parameter in the task function will receive the configuration dictionary, which is defined in the `Experiment` object.\n",
    "\n",
    "```python\n",
    "@task\n",
    "def my_function(input, config):\n",
    "    ...\n",
    "\n",
    "Experiment(... task=my_function, config={\"model\": \"gpt-4o-mini\", \"temperature\": 1.4}) # This is how you define the configuration in the experiment\n",
    "```\n",
    "\n",
    "## Why would you want to use a config?\n",
    "\n",
    "It will help organize your experiments better. With clearly defined parameters, we can show you in the UI more insights when you submit configurations, like advanced filtering and correlations between your evals and your config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a new experiment to get better results, this time we'll add a system prompt to the model and few shot examples.\n",
    "@dne.task\n",
    "def generate_capital_name(input, config):\n",
    "    output = client.chat.completions.create(\n",
    "        model=f\"{config.get(\"model\")}\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You will respond only with the name of the capital of the country, nothing else.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Paris\"},\n",
    "            {\"role\": \"user\", \"content\": input}],\n",
    "        temperature=config.get(\"temperature\", 0)\n",
    "    )\n",
    "    return output.choices[0].message.content\n",
    "\n",
    "experiment = dne.Experiment(\n",
    "    name=\"gpt-4o-mini-with-config\", \n",
    "    dataset=dataset, \n",
    "    task=generate_capital_name,\n",
    "    evaluators=[exact_match, contains_answer],\n",
    "    config={\"model\": \"gpt-4o-mini\", \"temperature\": 0}\n",
    ")\n",
    "\n",
    "results = experiment.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP**: If you come across errors when running experiments, you can immediately raise an error by providing the `raise_errors` argument to one of the following experiment methods: `run()`, `run_task()`, and `run_evaluators()`. e.g., `experiment.run(raise_errors=True)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Awesome! 🎉\n",
    "\n",
    "We've just created three experiments with different models and configurations.\n",
    "\n",
    "This is just the beginning, this task is very simple and it's not the best way to evaluate models, but it's a good starting point to understand how to use the SDK.\n",
    "\n",
    "In the next notebook we'll cover how to use a more complex task to evaluate models with more nuanced evaluators.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Feel free to play around with datasets, tasks, and evaluators to get a better understanding of how to use the SDK."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
