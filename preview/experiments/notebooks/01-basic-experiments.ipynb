{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running an experiment ðŸ§ª\n",
    "\n",
    "In this notebook, we'll walk through how to run a basic experiment.\n",
    "\n",
    "An experiment consists of three key components: a dataset, a task, and evaluators.\n",
    "- **Task**: A function that takes an input and generates a response.\n",
    "- **Evaluators**: Functions that compare the model's output against the expected output (along with the input) and returns a value based on specific criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's initialize the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from the .env file.\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from typing import Dict, Any\n",
    "\n",
    "from ddtrace.llmobs import LLMObs\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "LLMObs.enable(api_key=os.getenv(\"DD_API_KEY\"), app_key=os.getenv(\"DD_APPLICATION_KEY\"),  project_name=\"Onboarding\", ml_app=\"Onboarding-ML-App\")\n",
    "\n",
    "oai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will reuse the `capitals-of-the-world` dataset we created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pull the dataset from Datadog\n",
    "dataset = LLMObs.pull_dataset(name=\"capitals-of-the-world\")\n",
    "dataset.as_dataframe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Task & Evaluator\n",
    "\n",
    "This is the core of the experiment.\n",
    "\n",
    "First, we will define the task we want to evaluate.\n",
    "\n",
    "You can evaluate anything from single prompts to complex agents inside this function. The task and evaluators will be executed row-wise.\n",
    "\n",
    "Evaluators are functions that assess the modelâ€™s performance by comparing the expected output with the actual output generated by the model. They receive the following parameters:\n",
    "- input â€“ The original input prompt.\n",
    "- output â€“ The model's generated response.\n",
    "- expected_output â€“ The correct answer from the dataset.\n",
    "\n",
    "The evaluator should return a score or assessment based on a defined criterion.\n",
    "\n",
    "#### Example 1:\n",
    "For this experiment, we will ask the LLM the question with some predefined configuration around the model and temperature, take the answer as is, and create a simple equality check as well as an inclusiveness check to determine whether the modelâ€™s response matches the expected answer or contains the answer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# the task function will accept a row of input and will manipulate against it using the config provided\n",
    "def generate_capital(input_data: Dict[str, Any], config: Dict[str, Any]) -> str:\n",
    "    output = oai_client.chat.completions.create(\n",
    "        model=config[\"model\"],\n",
    "        messages=[{\"role\": \"user\", \"content\": input_data[\"question\"]}],\n",
    "        temperature=config[\"temperature\"]\n",
    "    )\n",
    "\n",
    "    return output.choices[0].message.content\n",
    "\n",
    "# Evaluators receive `input_data`, `output_data` (the output to test against), and `expected_output` (ground truth). All of them come automatically from the dataset and the task.\n",
    "# You can modify the logic to support different evaluation methods like fuzzy matching, semantic similarity, llm-as-a-judge, etc.\n",
    "def exact_match(input_data, output_data, expected_output):\n",
    "    return expected_output == output_data\n",
    "\n",
    "def contains_answer(input_data, output_data, expected_output):\n",
    "    return expected_output in output_data\n",
    "\n",
    "# We now define the experiment with a descriptive name, dataset, task function, and evaluators.\n",
    "experiment = LLMObs.experiment(\n",
    "    name=\"generate-capital-with-config\",\n",
    "    dataset=dataset,\n",
    "    task=generate_capital,\n",
    "    evaluators=[exact_match, contains_answer],\n",
    "    config={\"model\": \"gpt-4.1-nano\", \"temperature\": 0},\n",
    "    description=\"a cool basic experiment with config\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We now can run the experiment! This will execute the task against the dataset in a row-wise manner concurrently so that it runs faster."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "results = experiment.run(jobs=5)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After the experiment run is complete, you can see it in Datadog (it may take a few seconds to be accessible)."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "experiment.url",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can see that exact matches will fail, as the LLM will answer in a sentence, but the contains_answer check does OK."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now let's refine the task to give a nicer answer to hopefully get us better evaluation results.\n",
    "\n",
    "#### Example 2:\n",
    "For this experiment, we will ask the LLM the question with the same configuration around the model and temperature, but the answer should only contain 1 word, and use the same simple equality check as well as the inclusiveness check to determine whether the modelâ€™s response matches the expected answer or contains the answer."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_capital_name_one_word(input_data: Dict[str, Any], config: Dict[str, Any]) -> str:\n",
    "    output = oai_client.chat.completions.create(\n",
    "        model=config[\"model\"],\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You will respond only with the name of the capital of the country, nothing else.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Paris\"},\n",
    "            {\"role\": \"user\", \"content\": input_data[\"question\"]}],\n",
    "        temperature=config[\"temperature\"]\n",
    "    )\n",
    "\n",
    "    return output.choices[0].message.content\n",
    "\n",
    "def exact_match(input_data, output_data, expected_output):\n",
    "    return expected_output == output_data\n",
    "\n",
    "def contains_answer(input_data, output_data, expected_output):\n",
    "    return expected_output in output_data\n",
    "\n",
    "experiment = LLMObs.experiment(\n",
    "    name=\"generate-one-word-capital-with-config\",\n",
    "    dataset=dataset,\n",
    "    task=generate_capital_name_one_word,\n",
    "    evaluators=[exact_match, contains_answer],\n",
    "    config={\"model\": \"gpt-4.1-nano\", \"temperature\": 0},\n",
    "    description=\"a cool basic experiment with config\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And run the experiment again..."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "experiment.run(jobs=2)\n",
    "\n",
    "experiment.url"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After a few seconds, you should see that in the experiment in Datadog that we have matching responses!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**TIP**: If you come across errors when running experiments, you can immediately raise an error by providing the `raise_errors` argument to the `run()` method. e.g., `experiment.run(raise_errors=True)`."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Awesome! ðŸŽ‰\n",
    "\n",
    "We've just created two experiments with different results! This is just the beginning, this task is very simple and it's not the best way to evaluate models, but it's a good starting point to understand how to use the SDK.\n",
    "\n",
    "In the next notebook we'll cover how to use a more complex task to evaluate models with more nuanced evaluators.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Feel free to play around with datasets, tasks, and evaluators to get a better understanding of how to use the SDK."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
