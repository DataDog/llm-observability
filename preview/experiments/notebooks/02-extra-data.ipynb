{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Additional Input Data ðŸ“¥\n",
    "\n",
    "In some cases, a single input value isn't enoughâ€”you may need to process multiple values for a task.\n",
    "\n",
    "For this example, we'll use a Taskmaster dataset which contains comments and topics. Our goal is to evaluate whether a given comment is relevant to the list of topics.\n",
    "\n",
    "Additionally, your model's output may need to include multiple values. In this case, instead of just returning a label (e.g., \"in-topic\" or \"off-topic\"), we might also want to return a confidence score."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from the .env file.\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from typing import Dict, Any\n",
    "\n",
    "from ddtrace.llmobs import LLMObs\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "LLMObs.enable(api_key=os.getenv(\"DD_API_KEY\"), app_key=os.getenv(\"DD_APPLICATION_KEY\"),  project_name=\"Onboarding\", ml_app=\"Onboarding-ML-App\")\n",
    "\n",
    "oai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Specify the columns that contain the input and expected output.\n",
    "dataset = LLMObs.create_dataset_from_csv(csv_path=\"./data/taskmaster.csv\", dataset_name=\"taskmaster-mini\", input_data_columns=[\"prompt\", \"topics\"], expected_output_columns=[\"labels\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# If the label is False, then the comment is in topic with the list of topics. Otherwise, it is not.\n",
    "dataset.as_dataframe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task definition\n",
    "\n",
    "The following task will try to analyze whether a prompt belongs to a set of topics, both defined in the dataset.\n",
    "\n",
    "This approach will output multiple metrics and we will use them in the evaluators.\n",
    "\n",
    "The computation of certainty is a bit complex, so feel free to skip, it's not necessary to understand the workflow."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# a task that uses both the prompt and the topics from the input to determine if prompt is relevant\n",
    "def topic_relevance(input_data, config):\n",
    "    output = oai_client.chat.completions.create(\n",
    "        model=config['model'],\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"You are a {config['personality']} assistant that can detect if a comment is in topic with a given list of topics. Return YES if it is, otherwise return NO. Nothing else.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Comment: {input['prompt']}\\n\\nTopics: {input['topics']}\"}\n",
    "        ],\n",
    "        logprobs=True,\n",
    "        top_logprobs=10,\n",
    "        temperature=config[\"temperature\"]\n",
    "    )\n",
    "\n",
    "    response = output.choices[0].message.content == \"YES\"\n",
    "\n",
    "    # Get logprobs for YES and NO responses\n",
    "    logprobs = output.choices[0].logprobs.content[0].top_logprobs\n",
    "    yes_prob = next((lp.logprob for lp in logprobs if lp.token == \"YES\"), float(\"-inf\"))\n",
    "    no_prob = next((lp.logprob for lp in logprobs if lp.token == \"NO\"), float(\"-inf\"))\n",
    "\n",
    "    # Convert log probabilities to raw probabilities\n",
    "    yes_raw_prob = math.exp(yes_prob)\n",
    "    no_raw_prob = math.exp(no_prob)\n",
    "\n",
    "    # Normalize probabilities to get proper probability distribution\n",
    "    total_prob = yes_raw_prob + no_raw_prob\n",
    "    if total_prob > 0:  # Avoid division by zero\n",
    "        yes_norm_prob = yes_raw_prob / total_prob\n",
    "        no_norm_prob = no_raw_prob / total_prob\n",
    "    else:\n",
    "        # Fallback if both probabilities are extremely low\n",
    "        yes_norm_prob = 0.5 if yes_raw_prob > 0 else 0\n",
    "        no_norm_prob = 0.5 if no_raw_prob > 0 else 0\n",
    "\n",
    "    # Calculate normalized confidence for the chosen response\n",
    "    confidence = yes_norm_prob if response else no_norm_prob\n",
    "\n",
    "    # Calculate entropy-based certainty (1 = perfectly certain, 0 = completely uncertain)\n",
    "    if yes_norm_prob > 0 and no_norm_prob > 0:\n",
    "        entropy = -(yes_norm_prob * math.log2(yes_norm_prob) + no_norm_prob * math.log2(no_norm_prob))\n",
    "        max_entropy = 1.0  # Maximum entropy for binary choice\n",
    "        certainty = 1 - (entropy / max_entropy)\n",
    "    else:\n",
    "        certainty = 1.0  # If one probability is 0, the model is completely certain\n",
    "\n",
    "    return {\n",
    "        \"response\": str(not response),  # Maintaining your original logic\n",
    "        \"confidence\": confidence,       # Normalized probability of chosen answer\n",
    "        \"certainty\": certainty,         # Entropy-based measure of model certainty\n",
    "        \"yes_probability\": yes_norm_prob,\n",
    "        \"no_probability\": no_norm_prob,\n",
    "        \"raw_confidence\": math.exp(yes_prob if response else no_prob)  # Original calculation for comparison\n",
    "    }\n",
    "\n",
    "\n",
    "def exact_match(input_data, output_data, expected_output):\n",
    "    return expected_output == output[\"response\"]\n",
    "\n",
    "# define a confidence score evaluator to check if the confidence score is greater than 0.8 and the output is not the expected output.\n",
    "def false_confidence(input_data, output_data, expected_output):\n",
    "    return output[\"certainty\"] > 0.8 and expected_output != output[\"response\"]\n",
    "\n",
    "\n",
    "experiment = LLMObs.experiment(\n",
    "    name=\"taskmaster-experiment\",\n",
    "    dataset=dataset,\n",
    "    task=topic_relevance,\n",
    "    evaluators=[exact_match, false_confidence],\n",
    "    config={\"model\": \"gpt-4o-mini\", \"temperature\": 0.3, \"personality\": \"helpful\"},\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Let's test just on one sample\n",
    "input = dataset[0][\"input_data\"]\n",
    "output = topic_relevance(input, {\"model\": \"gpt-4o-mini\", \"temperature\": 0.3, \"personality\": \"helpful\"})\n",
    "print(output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's run the experiment."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = experiment.run(jobs=50)\n",
    "\n",
    "experiment.url"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "If you check the experiment in Datadog with the link above (may take a few seconds to be accessible), you'll be able to see the experiment with a dataset that is imported from a CSV, using a more complex evaluator!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
