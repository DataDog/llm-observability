{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "\n",
    "The setup below shows in-code configuration. For most applications, we can also enable LLMObs simply by calling `ddtrace-run` with the appropriate env vars, [as seen here in our quickstart instructions](https://docs.datadoghq.com/tracing/llm_observability/quickstart/).\n",
    "\n",
    "The code below requires you to have already created an `.env` file with several configuration variables as explained in the README.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from ddtrace.llmobs import LLMObs\n",
    "\n",
    "LLMObs.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note for enterprise customers using secrets:**\n",
    "\n",
    "If you are using secrets, you can enable LLM Observability with more specific parameters as demonstrated below.\n",
    "\n",
    "```python\n",
    "LLMObs.enable(\n",
    "  ml_app=\"<YOUR_ML_APP_NAME>\",\n",
    "  api_key=\"<YOUR_DATADOG_API_KEY>\",\n",
    "  site=\"<YOUR_DATADOG_SITE>\",\n",
    "  agentless_enabled=True,\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing an LLM Call\n",
    "\n",
    "An LLM Span represents a call to a model. In this simple example, we are asking `gpt-3.5-turbo` to summarize a provided text and identify a list of topics from the text.\n",
    "\n",
    "Because we use OpenAI, the call to the LLM is instrumented automatically by Datadog, with no further action required on our part:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "oai_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "sys_prompt = \"\"\"\n",
    "Your task is to \n",
    "1. Summarize the given text at a 6th grade reading level in no more than 2 sentences.\n",
    "2. Identify what topics the text belongs to that would allow you to categorize it in a school library.\n",
    "Format your output strictly following this JSON convention:\n",
    "{\t\n",
    "    \"topics\": <[insert array of topics here]>\n",
    "    \"summary\": <insert summary here>\n",
    "}\n",
    "\t\"\"\"\n",
    "\n",
    "\n",
    "def summarize(text, prompt=sys_prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    # llm span auto-instrumented via our openai integration\n",
    "    response_content = (\n",
    "        oai_client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "    return json.loads(response_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "ONE JANUARY day, thirty years ago, the little town of Hanover, anchored on a windy Nebraska tableland, was trying not to be blown away. A mist of fine snowflakes was curling and eddying about the cluster of low drab buildings huddled on the gray prairie, under a gray sky. The dwelling-houses were set about haphazard on the tough prairie sod; some of them looked as if they had been moved in overnight, and others as if they were straying off by themselves, headed straight for the open plain. None of them had any appearance of permanence, and the howling wind blew under them as well as over them. The main street was a deeply rutted road, now frozen hard, which ran from the squat red railway station and the grain \"elevator\" at the north end of the town to the lumber yard and the horse pond at the south end. On either side of this road straggled two uneven rows of wooden buildings; the general merchandise stores, the two banks, the drug store, the feed store, the saloon, the post-office. The board sidewalks were gray with trampled snow, but at two o'clock in the afternoon the shopkeepers, having come back from dinner, were keeping well behind their frosty windows.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topics': ['History', 'Rural Life', 'Weather Phenomena'],\n",
       " 'summary': \"In a small Nebraska town thirty years ago, the residents struggle against harsh winter weather conditions as fine snowflakes swirl around the low buildings. The town's main street, lined with various stores and businesses, is quiet in the afternoon as the shopkeepers stay inside to keep warm.\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the trace in Datadog\n",
    "\n",
    "Now, check out the [LLM Observability interface](https://app.datadoghq.com/llm) in Datadog. You should see a trace that describes the LLM call, including the system prompt, the user prompt, and the response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional resources\n",
    "\n",
    "- [List of all integrations supported by Datadog's LLM Observability product](https://docs.datadoghq.com/tracing/llm_observability/sdk/#llm-integrations)\n",
    "- [Instructions for manually instrumenting an LLM Span](https://docs.datadoghq.com/tracing/llm_observability/sdk/#llm-span)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
